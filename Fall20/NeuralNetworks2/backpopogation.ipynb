{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss function: L'(W1, b1, W2, b2).\n",
    "def L_prime(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L'(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the gradients: dL/dW1 (Shape: [2,3]), dL/db1 (Shape: [3,1]),\n",
    "                          dL/dW2 (Shape: [3,1]), dL/db2 (Shape: [1,1]).\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "\n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                          # Shape: [n, 3].\n",
    "    P = Y*(W2.T.dot(H.T)+b2).T                           # Shape: [n, 1].\n",
    "#     print(P.shape)\n",
    "    # Calculate the gradients: dL/dW1, dL/db1, dL/dW2, dL/db2.\n",
    "    dL_by_dW2 = H.T.dot((P-1)*Y)                            # Shape: [3,1].\n",
    "    \n",
    "#     dL_by_db2 =  (P-1).T.dot(Y)                           # Shape: [1,1].\n",
    "    dL_by_db2 = np.ones((n,1)).T.dot((P-1)*Y)\n",
    "    \n",
    "#     print(W2.shape)\n",
    "    dL_by_dH  = ((P-1)*Y).dot(W2.T)                           # Shape: [n,3].\n",
    "    dL_by_dW1  = X.T.dot(dL_by_dH*H*(1-H))                   # Shape: [2,3].\n",
    "#     print(dL_by_dW1.shape)\n",
    "    dL_by_db1  = (dL_by_dH*H*(1-H)).T.dot(np.ones((n,1)))                        # Shape: [3,1].\n",
    "#     print(dL_by_db1.shape)\n",
    "    return dL_by_dW1, dL_by_db1, dL_by_dW2, dL_by_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def L(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the loss.       Shape: Scalar.\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "#     print(X.shape)\n",
    "    \n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                             # Shape: [n, 3].\n",
    "#     print(H.shape)\n",
    "#     print(W2.T.dot(H.T).shape)\n",
    "    P = sigmoid(Y*(W2.T.dot(H.T)+b2).T)                             # Shape: [n, 1].\n",
    "    \n",
    "#     print((W2.T.dot(H.T)+b2).shape)\n",
    "#     print(P.shape)\n",
    "    # Get the loss.\n",
    "    L =    -np.sum(np.log(P))                        # Shape: Scalar.\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# supposed to find where loss is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0025\n",
    "n_iter = 200000                        # Number of iterations\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2,3)/((2*3)**2)   # Weight matrix 1.\n",
    "b1 = np.random.randn(3,1)/((3*1)**2)   # Bias vector 1.\n",
    "W2 = np.random.randn(3,1)/((3*1)**2)   # Weight matrix 2.\n",
    "b2 = np.random.randn(1,1)/((1*1)**2)   # Bias vector 2.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, Y_train, W1, b1, W2, b2)]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # gradient descent \n",
    "    \n",
    "    gradient_W1, gradient_b1, gradient_W2, gradient_b2 = \\\n",
    "        L_prime(X_train, Y_train, W1, b1, W2, b2)\n",
    "    \n",
    "    W1_new = W1 - learning_rate * gradient_W1\n",
    "    b1_new = b1 - learning_rate * gradient_b1\n",
    "    W2_new = W2 - learning_rate * gradient_W2\n",
    "    b2_new = b2 - learning_rate * gradient_b2\n",
    "    \n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, Y_train, W1_new, b1_new, W2_new, b2_new))\n",
    "    \n",
    "    # L1-norm of weight/bias changing.\n",
    "    norm = np.abs(W1_new-W1).sum() + np.abs(b1_new-b1).sum() + \\\n",
    "           np.abs(W2_new-W2).sum() + np.abs(b2_new-b2).sum() \n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print('i: {:6d} L: {:.3f} norm:{:.6f}'.format(i, L_list[-1], norm))\n",
    "        \n",
    "    W1 = W1_new\n",
    "    b1 = b1_new\n",
    "    W2 = W2_new\n",
    "    b2 = b2_new\n",
    "    \n",
    "print ('W1 matrix: \\n' + str(W1))\n",
    "print ('b1 vector: \\n' + str(b1))\n",
    "print ('W2 matrix: \\n' + str(W2))\n",
    "print ('b2 vector: \\n' + str(b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropogation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer=tf.train.GradientDescentOptimizer(0.01).minimize(f_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
