{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss function: L'(W1, b1, W2, b2).\n",
    "def L_prime(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L'(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the gradients: dL/dW1 (Shape: [2,3]), dL/db1 (Shape: [3,1]),\n",
    "                          dL/dW2 (Shape: [3,1]), dL/db2 (Shape: [1,1]).\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "\n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                          # Shape: [n, 3].\n",
    "    P = Y*(W2.T.dot(H.T)+b2).T                           # Shape: [n, 1].\n",
    "#     print(P.shape)\n",
    "    # Calculate the gradients: dL/dW1, dL/db1, dL/dW2, dL/db2.\n",
    "    dL_by_dW2 = H.T.dot((P-1)*Y)                            # Shape: [3,1].\n",
    "    \n",
    "#     dL_by_db2 =  (P-1).T.dot(Y)                           # Shape: [1,1].\n",
    "    dL_by_db2 = np.ones((n,1)).T.dot((P-1)*Y)\n",
    "    \n",
    "#     print(W2.shape)\n",
    "    dL_by_dH  = ((P-1)*Y).dot(W2.T)                           # Shape: [n,3].\n",
    "    dL_by_dW1  = X.T.dot(dL_by_dH*H*(1-H))                   # Shape: [2,3].\n",
    "#     print(dL_by_dW1.shape)\n",
    "    dL_by_db1  = (dL_by_dH*H*(1-H)).T.dot(np.ones((n,1)))                        # Shape: [3,1].\n",
    "#     print(dL_by_db1.shape)\n",
    "    return dL_by_dW1, dL_by_db1, dL_by_dW2, dL_by_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def L(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the loss.       Shape: Scalar.\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "#     print(X.shape)\n",
    "    \n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                             # Shape: [n, 3].\n",
    "#     print(H.shape)\n",
    "#     print(W2.T.dot(H.T).shape)\n",
    "    P = sigmoid(Y*(W2.T.dot(H.T)+b2).T)                             # Shape: [n, 1].\n",
    "    \n",
    "#     print((W2.T.dot(H.T)+b2).shape)\n",
    "#     print(P.shape)\n",
    "    # Get the loss.\n",
    "    L =    -np.sum(np.log(P))                        # Shape: Scalar.\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate some data using a function mapping from R^2 -> R^1 (2d coordinates to scalar values)\n",
    "def generate_data():\n",
    "    \n",
    "    # generates 1000 ordered data points from 0 to 1 with a bit of noise using random.uniform\n",
    "    def generate_linear_noisy():\n",
    "        return np.linspace(0, 1, num=1000) + np.random.uniform(-0.05, 0.05, (1000,))\n",
    "    \n",
    "    X_train = np.array([generate_linear_noisy(), generate_linear_noisy()]).T\n",
    "    \n",
    "    # the function modeled here is F(x, y) -> x / 2 + y / 2\n",
    "    Y_train = (X_train[:,0] * 0.5 + X_train[:,1] * 0.5).reshape(1000, 1)\n",
    "    return X_train, Y_train\n",
    "X_train, Y_train = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04739148  0.03284548]\n",
      " [-0.01272119 -0.0220772 ]\n",
      " [ 0.0338837  -0.03660984]\n",
      " ...\n",
      " [ 1.04537137  1.00214967]\n",
      " [ 1.01571349  0.97313234]\n",
      " [ 0.97557795  0.9906448 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# supposed to find where loss is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:      0 L: 498.674 norm:0.055257\n",
      "i:    500 L: 406.410 norm:0.002239\n",
      "i:   1000 L: 403.164 norm:0.003733\n",
      "i:   1500 L: 396.925 norm:0.005403\n",
      "i:   2000 L: 386.843 norm:0.006417\n",
      "i:   2500 L: 374.793 norm:0.005971\n",
      "i:   3000 L: 364.575 norm:0.004444\n",
      "i:   3500 L: 357.869 norm:0.002940\n",
      "i:   4000 L: 353.861 norm:0.002231\n",
      "i:   4500 L: 351.339 norm:0.001968\n",
      "i:   5000 L: 349.543 norm:0.001782\n",
      "i:   5500 L: 348.109 norm:0.001648\n",
      "i:   6000 L: 346.888 norm:0.001552\n",
      "i:   6500 L: 345.816 norm:0.001480\n",
      "i:   7000 L: 344.861 norm:0.001408\n",
      "i:   7500 L: 344.005 norm:0.001337\n",
      "i:   8000 L: 343.233 norm:0.001268\n",
      "i:   8500 L: 342.533 norm:0.001202\n",
      "i:   9000 L: 341.897 norm:0.001137\n",
      "i:   9500 L: 341.317 norm:0.001076\n",
      "i:  10000 L: 340.786 norm:0.001018\n",
      "i:  10500 L: 340.299 norm:0.000964\n",
      "i:  11000 L: 339.850 norm:0.000914\n",
      "i:  11500 L: 339.436 norm:0.000873\n",
      "i:  12000 L: 339.053 norm:0.000836\n",
      "i:  12500 L: 338.697 norm:0.000802\n",
      "i:  13000 L: 338.366 norm:0.000770\n",
      "i:  13500 L: 338.057 norm:0.000740\n",
      "i:  14000 L: 337.767 norm:0.000712\n",
      "i:  14500 L: 337.495 norm:0.000686\n",
      "i:  15000 L: 337.239 norm:0.000661\n",
      "i:  15500 L: 336.997 norm:0.000638\n",
      "i:  16000 L: 336.768 norm:0.000616\n",
      "i:  16500 L: 336.551 norm:0.000596\n",
      "i:  17000 L: 336.345 norm:0.000578\n",
      "i:  17500 L: 336.148 norm:0.000561\n",
      "i:  18000 L: 335.961 norm:0.000546\n",
      "i:  18500 L: 335.782 norm:0.000531\n",
      "i:  19000 L: 335.610 norm:0.000517\n",
      "i:  19500 L: 335.446 norm:0.000503\n",
      "i:  20000 L: 335.288 norm:0.000490\n",
      "i:  20500 L: 335.137 norm:0.000477\n",
      "i:  21000 L: 334.991 norm:0.000465\n",
      "i:  21500 L: 334.850 norm:0.000453\n",
      "i:  22000 L: 334.714 norm:0.000442\n",
      "i:  22500 L: 334.583 norm:0.000431\n",
      "i:  23000 L: 334.456 norm:0.000421\n",
      "i:  23500 L: 334.333 norm:0.000410\n",
      "i:  24000 L: 334.214 norm:0.000401\n",
      "i:  24500 L: 334.099 norm:0.000391\n",
      "i:  25000 L: 333.987 norm:0.000382\n",
      "i:  25500 L: 333.879 norm:0.000374\n",
      "i:  26000 L: 333.773 norm:0.000366\n",
      "i:  26500 L: 333.670 norm:0.000359\n",
      "i:  27000 L: 333.570 norm:0.000353\n",
      "i:  27500 L: 333.472 norm:0.000348\n",
      "i:  28000 L: 333.377 norm:0.000343\n",
      "i:  28500 L: 333.284 norm:0.000338\n",
      "i:  29000 L: 333.194 norm:0.000335\n",
      "i:  29500 L: 333.105 norm:0.000332\n",
      "i:  30000 L: 333.018 norm:0.000330\n",
      "i:  30500 L: 332.934 norm:0.000329\n",
      "i:  31000 L: 332.851 norm:0.000328\n",
      "i:  31500 L: 332.769 norm:0.000326\n",
      "i:  32000 L: 332.690 norm:0.000325\n",
      "i:  32500 L: 332.612 norm:0.000324\n",
      "i:  33000 L: 332.535 norm:0.000323\n",
      "i:  33500 L: 332.460 norm:0.000322\n",
      "i:  34000 L: 332.386 norm:0.000322\n",
      "i:  34500 L: 332.313 norm:0.000321\n",
      "i:  35000 L: 332.242 norm:0.000320\n",
      "i:  35500 L: 332.172 norm:0.000319\n",
      "i:  36000 L: 332.103 norm:0.000319\n",
      "i:  36500 L: 332.035 norm:0.000318\n",
      "i:  37000 L: 331.968 norm:0.000318\n",
      "i:  37500 L: 331.902 norm:0.000317\n",
      "i:  38000 L: 331.838 norm:0.000317\n",
      "i:  38500 L: 331.774 norm:0.000316\n",
      "i:  39000 L: 331.711 norm:0.000316\n",
      "i:  39500 L: 331.649 norm:0.000315\n",
      "i:  40000 L: 331.588 norm:0.000315\n",
      "i:  40500 L: 331.528 norm:0.000314\n",
      "i:  41000 L: 331.468 norm:0.000314\n",
      "i:  41500 L: 331.410 norm:0.000313\n",
      "i:  42000 L: 331.352 norm:0.000313\n",
      "i:  42500 L: 331.295 norm:0.000312\n",
      "i:  43000 L: 331.238 norm:0.000311\n",
      "i:  43500 L: 331.183 norm:0.000311\n",
      "i:  44000 L: 331.128 norm:0.000310\n",
      "i:  44500 L: 331.074 norm:0.000309\n",
      "i:  45000 L: 331.020 norm:0.000308\n",
      "i:  45500 L: 330.967 norm:0.000307\n",
      "i:  46000 L: 330.915 norm:0.000306\n",
      "i:  46500 L: 330.864 norm:0.000305\n",
      "i:  47000 L: 330.813 norm:0.000304\n",
      "i:  47500 L: 330.763 norm:0.000302\n",
      "i:  48000 L: 330.713 norm:0.000301\n",
      "i:  48500 L: 330.664 norm:0.000300\n",
      "i:  49000 L: 330.616 norm:0.000298\n",
      "i:  49500 L: 330.568 norm:0.000296\n",
      "i:  50000 L: 330.521 norm:0.000295\n",
      "i:  50500 L: 330.475 norm:0.000293\n",
      "i:  51000 L: 330.429 norm:0.000291\n",
      "i:  51500 L: 330.384 norm:0.000289\n",
      "i:  52000 L: 330.339 norm:0.000287\n",
      "i:  52500 L: 330.295 norm:0.000285\n",
      "i:  53000 L: 330.251 norm:0.000283\n",
      "i:  53500 L: 330.208 norm:0.000281\n",
      "i:  54000 L: 330.166 norm:0.000279\n",
      "i:  54500 L: 330.124 norm:0.000277\n",
      "i:  55000 L: 330.083 norm:0.000274\n",
      "i:  55500 L: 330.042 norm:0.000272\n",
      "i:  56000 L: 330.001 norm:0.000270\n",
      "i:  56500 L: 329.962 norm:0.000267\n",
      "i:  57000 L: 329.922 norm:0.000265\n",
      "i:  57500 L: 329.883 norm:0.000262\n",
      "i:  58000 L: 329.845 norm:0.000260\n",
      "i:  58500 L: 329.807 norm:0.000257\n",
      "i:  59000 L: 329.770 norm:0.000255\n",
      "i:  59500 L: 329.733 norm:0.000253\n",
      "i:  60000 L: 329.696 norm:0.000250\n",
      "i:  60500 L: 329.660 norm:0.000248\n",
      "i:  61000 L: 329.625 norm:0.000245\n",
      "i:  61500 L: 329.589 norm:0.000243\n",
      "i:  62000 L: 329.555 norm:0.000240\n",
      "i:  62500 L: 329.520 norm:0.000238\n",
      "i:  63000 L: 329.486 norm:0.000235\n",
      "i:  63500 L: 329.453 norm:0.000233\n",
      "i:  64000 L: 329.419 norm:0.000231\n",
      "i:  64500 L: 329.387 norm:0.000228\n",
      "i:  65000 L: 329.354 norm:0.000226\n",
      "i:  65500 L: 329.322 norm:0.000224\n",
      "i:  66000 L: 329.290 norm:0.000221\n",
      "i:  66500 L: 329.259 norm:0.000219\n",
      "i:  67000 L: 329.228 norm:0.000217\n",
      "i:  67500 L: 329.197 norm:0.000215\n",
      "i:  68000 L: 329.167 norm:0.000212\n",
      "i:  68500 L: 329.137 norm:0.000210\n",
      "i:  69000 L: 329.107 norm:0.000208\n",
      "i:  69500 L: 329.078 norm:0.000206\n",
      "i:  70000 L: 329.048 norm:0.000204\n",
      "i:  70500 L: 329.020 norm:0.000202\n",
      "i:  71000 L: 328.991 norm:0.000200\n",
      "i:  71500 L: 328.963 norm:0.000199\n",
      "i:  72000 L: 328.935 norm:0.000197\n",
      "i:  72500 L: 328.907 norm:0.000196\n",
      "i:  73000 L: 328.880 norm:0.000194\n",
      "i:  73500 L: 328.853 norm:0.000193\n",
      "i:  74000 L: 328.826 norm:0.000191\n",
      "i:  74500 L: 328.799 norm:0.000190\n",
      "i:  75000 L: 328.773 norm:0.000188\n",
      "i:  75500 L: 328.747 norm:0.000187\n",
      "i:  76000 L: 328.721 norm:0.000185\n",
      "i:  76500 L: 328.696 norm:0.000184\n",
      "i:  77000 L: 328.670 norm:0.000183\n",
      "i:  77500 L: 328.645 norm:0.000181\n",
      "i:  78000 L: 328.621 norm:0.000180\n",
      "i:  78500 L: 328.596 norm:0.000178\n",
      "i:  79000 L: 328.572 norm:0.000177\n",
      "i:  79500 L: 328.548 norm:0.000176\n",
      "i:  80000 L: 328.524 norm:0.000175\n",
      "i:  80500 L: 328.500 norm:0.000173\n",
      "i:  81000 L: 328.477 norm:0.000172\n",
      "i:  81500 L: 328.453 norm:0.000171\n",
      "i:  82000 L: 328.430 norm:0.000170\n",
      "i:  82500 L: 328.407 norm:0.000168\n",
      "i:  83000 L: 328.385 norm:0.000167\n",
      "i:  83500 L: 328.362 norm:0.000166\n",
      "i:  84000 L: 328.340 norm:0.000165\n",
      "i:  84500 L: 328.318 norm:0.000164\n",
      "i:  85000 L: 328.296 norm:0.000163\n",
      "i:  85500 L: 328.275 norm:0.000162\n",
      "i:  86000 L: 328.253 norm:0.000161\n",
      "i:  86500 L: 328.232 norm:0.000160\n",
      "i:  87000 L: 328.211 norm:0.000159\n",
      "i:  87500 L: 328.190 norm:0.000157\n",
      "i:  88000 L: 328.170 norm:0.000156\n",
      "i:  88500 L: 328.149 norm:0.000155\n",
      "i:  89000 L: 328.129 norm:0.000154\n",
      "i:  89500 L: 328.109 norm:0.000154\n",
      "i:  90000 L: 328.089 norm:0.000153\n",
      "i:  90500 L: 328.069 norm:0.000152\n",
      "i:  91000 L: 328.049 norm:0.000151\n",
      "i:  91500 L: 328.030 norm:0.000150\n",
      "i:  92000 L: 328.011 norm:0.000150\n",
      "i:  92500 L: 327.991 norm:0.000149\n",
      "i:  93000 L: 327.973 norm:0.000148\n",
      "i:  93500 L: 327.954 norm:0.000148\n",
      "i:  94000 L: 327.935 norm:0.000147\n",
      "i:  94500 L: 327.917 norm:0.000146\n",
      "i:  95000 L: 327.898 norm:0.000146\n",
      "i:  95500 L: 327.880 norm:0.000145\n",
      "i:  96000 L: 327.862 norm:0.000144\n",
      "i:  96500 L: 327.844 norm:0.000144\n",
      "i:  97000 L: 327.826 norm:0.000143\n",
      "i:  97500 L: 327.809 norm:0.000142\n",
      "i:  98000 L: 327.791 norm:0.000142\n",
      "i:  98500 L: 327.774 norm:0.000141\n",
      "i:  99000 L: 327.757 norm:0.000141\n",
      "i:  99500 L: 327.740 norm:0.000140\n",
      "i: 100000 L: 327.723 norm:0.000139\n",
      "i: 100500 L: 327.706 norm:0.000139\n",
      "i: 101000 L: 327.690 norm:0.000138\n",
      "i: 101500 L: 327.673 norm:0.000137\n",
      "i: 102000 L: 327.657 norm:0.000137\n",
      "i: 102500 L: 327.641 norm:0.000136\n",
      "i: 103000 L: 327.625 norm:0.000136\n",
      "i: 103500 L: 327.609 norm:0.000135\n",
      "i: 104000 L: 327.593 norm:0.000135\n",
      "i: 104500 L: 327.577 norm:0.000134\n",
      "i: 105000 L: 327.561 norm:0.000134\n",
      "i: 105500 L: 327.546 norm:0.000133\n",
      "i: 106000 L: 327.531 norm:0.000132\n",
      "i: 106500 L: 327.515 norm:0.000132\n",
      "i: 107000 L: 327.500 norm:0.000131\n",
      "i: 107500 L: 327.485 norm:0.000131\n",
      "i: 108000 L: 327.470 norm:0.000130\n",
      "i: 108500 L: 327.456 norm:0.000130\n",
      "i: 109000 L: 327.441 norm:0.000129\n",
      "i: 109500 L: 327.426 norm:0.000129\n",
      "i: 110000 L: 327.412 norm:0.000128\n",
      "i: 110500 L: 327.397 norm:0.000128\n",
      "i: 111000 L: 327.383 norm:0.000127\n",
      "i: 111500 L: 327.369 norm:0.000127\n",
      "i: 112000 L: 327.355 norm:0.000126\n",
      "i: 112500 L: 327.341 norm:0.000126\n",
      "i: 113000 L: 327.327 norm:0.000125\n",
      "i: 113500 L: 327.314 norm:0.000125\n",
      "i: 114000 L: 327.300 norm:0.000124\n",
      "i: 114500 L: 327.286 norm:0.000124\n",
      "i: 115000 L: 327.273 norm:0.000123\n",
      "i: 115500 L: 327.260 norm:0.000123\n",
      "i: 116000 L: 327.246 norm:0.000123\n",
      "i: 116500 L: 327.233 norm:0.000122\n",
      "i: 117000 L: 327.220 norm:0.000122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 117500 L: 327.207 norm:0.000121\n",
      "i: 118000 L: 327.194 norm:0.000121\n",
      "i: 118500 L: 327.182 norm:0.000120\n",
      "i: 119000 L: 327.169 norm:0.000120\n",
      "i: 119500 L: 327.156 norm:0.000119\n",
      "i: 120000 L: 327.144 norm:0.000119\n",
      "i: 120500 L: 327.132 norm:0.000118\n",
      "i: 121000 L: 327.119 norm:0.000118\n",
      "i: 121500 L: 327.107 norm:0.000118\n",
      "i: 122000 L: 327.095 norm:0.000117\n",
      "i: 122500 L: 327.083 norm:0.000117\n",
      "i: 123000 L: 327.071 norm:0.000116\n",
      "i: 123500 L: 327.059 norm:0.000116\n",
      "i: 124000 L: 327.047 norm:0.000115\n",
      "i: 124500 L: 327.035 norm:0.000115\n",
      "i: 125000 L: 327.024 norm:0.000115\n",
      "i: 125500 L: 327.012 norm:0.000114\n",
      "i: 126000 L: 327.000 norm:0.000114\n",
      "i: 126500 L: 326.989 norm:0.000113\n",
      "i: 127000 L: 326.978 norm:0.000113\n",
      "i: 127500 L: 326.966 norm:0.000113\n",
      "i: 128000 L: 326.955 norm:0.000112\n",
      "i: 128500 L: 326.944 norm:0.000112\n",
      "i: 129000 L: 326.933 norm:0.000111\n",
      "i: 129500 L: 326.922 norm:0.000111\n",
      "i: 130000 L: 326.911 norm:0.000111\n",
      "i: 130500 L: 326.900 norm:0.000110\n",
      "i: 131000 L: 326.889 norm:0.000110\n",
      "i: 131500 L: 326.879 norm:0.000109\n",
      "i: 132000 L: 326.868 norm:0.000109\n",
      "i: 132500 L: 326.857 norm:0.000109\n",
      "i: 133000 L: 326.847 norm:0.000108\n",
      "i: 133500 L: 326.836 norm:0.000108\n",
      "i: 134000 L: 326.826 norm:0.000108\n",
      "i: 134500 L: 326.816 norm:0.000107\n",
      "i: 135000 L: 326.805 norm:0.000107\n",
      "i: 135500 L: 326.795 norm:0.000106\n",
      "i: 136000 L: 326.785 norm:0.000106\n",
      "i: 136500 L: 326.775 norm:0.000106\n",
      "i: 137000 L: 326.765 norm:0.000105\n",
      "i: 137500 L: 326.755 norm:0.000105\n",
      "i: 138000 L: 326.745 norm:0.000105\n",
      "i: 138500 L: 326.735 norm:0.000104\n",
      "i: 139000 L: 326.726 norm:0.000104\n",
      "i: 139500 L: 326.716 norm:0.000104\n",
      "i: 140000 L: 326.706 norm:0.000103\n",
      "i: 140500 L: 326.697 norm:0.000103\n",
      "i: 141000 L: 326.687 norm:0.000103\n",
      "i: 141500 L: 326.678 norm:0.000102\n",
      "i: 142000 L: 326.668 norm:0.000102\n",
      "i: 142500 L: 326.659 norm:0.000102\n",
      "i: 143000 L: 326.650 norm:0.000101\n",
      "i: 143500 L: 326.640 norm:0.000101\n",
      "i: 144000 L: 326.631 norm:0.000101\n",
      "i: 144500 L: 326.622 norm:0.000100\n",
      "i: 145000 L: 326.613 norm:0.000100\n",
      "i: 145500 L: 326.604 norm:0.000100\n",
      "i: 146000 L: 326.595 norm:0.000099\n",
      "i: 146500 L: 326.586 norm:0.000099\n",
      "i: 147000 L: 326.577 norm:0.000099\n",
      "i: 147500 L: 326.568 norm:0.000098\n",
      "i: 148000 L: 326.559 norm:0.000098\n",
      "i: 148500 L: 326.551 norm:0.000098\n",
      "i: 149000 L: 326.542 norm:0.000097\n",
      "i: 149500 L: 326.533 norm:0.000097\n",
      "i: 150000 L: 326.525 norm:0.000097\n",
      "i: 150500 L: 326.516 norm:0.000096\n",
      "i: 151000 L: 326.508 norm:0.000096\n",
      "i: 151500 L: 326.499 norm:0.000096\n",
      "i: 152000 L: 326.491 norm:0.000095\n",
      "i: 152500 L: 326.483 norm:0.000095\n",
      "i: 153000 L: 326.474 norm:0.000095\n",
      "i: 153500 L: 326.466 norm:0.000095\n",
      "i: 154000 L: 326.458 norm:0.000094\n",
      "i: 154500 L: 326.450 norm:0.000094\n",
      "i: 155000 L: 326.442 norm:0.000094\n",
      "i: 155500 L: 326.434 norm:0.000093\n",
      "i: 156000 L: 326.426 norm:0.000093\n",
      "i: 156500 L: 326.418 norm:0.000093\n",
      "i: 157000 L: 326.410 norm:0.000093\n",
      "i: 157500 L: 326.402 norm:0.000092\n",
      "i: 158000 L: 326.394 norm:0.000092\n",
      "i: 158500 L: 326.386 norm:0.000092\n",
      "i: 159000 L: 326.378 norm:0.000091\n",
      "i: 159500 L: 326.371 norm:0.000091\n",
      "i: 160000 L: 326.363 norm:0.000091\n",
      "i: 160500 L: 326.355 norm:0.000091\n",
      "i: 161000 L: 326.348 norm:0.000090\n",
      "i: 161500 L: 326.340 norm:0.000090\n",
      "i: 162000 L: 326.333 norm:0.000090\n",
      "i: 162500 L: 326.325 norm:0.000089\n",
      "i: 163000 L: 326.318 norm:0.000089\n",
      "i: 163500 L: 326.310 norm:0.000089\n",
      "i: 164000 L: 326.303 norm:0.000089\n",
      "i: 164500 L: 326.296 norm:0.000088\n",
      "i: 165000 L: 326.288 norm:0.000088\n",
      "i: 165500 L: 326.281 norm:0.000088\n",
      "i: 166000 L: 326.274 norm:0.000088\n",
      "i: 166500 L: 326.267 norm:0.000087\n",
      "i: 167000 L: 326.259 norm:0.000087\n",
      "i: 167500 L: 326.252 norm:0.000087\n",
      "i: 168000 L: 326.245 norm:0.000087\n",
      "i: 168500 L: 326.238 norm:0.000086\n",
      "i: 169000 L: 326.231 norm:0.000086\n",
      "i: 169500 L: 326.224 norm:0.000086\n",
      "i: 170000 L: 326.217 norm:0.000086\n",
      "i: 170500 L: 326.211 norm:0.000085\n",
      "i: 171000 L: 326.204 norm:0.000085\n",
      "i: 171500 L: 326.197 norm:0.000085\n",
      "i: 172000 L: 326.190 norm:0.000085\n",
      "i: 172500 L: 326.183 norm:0.000084\n",
      "i: 173000 L: 326.177 norm:0.000084\n",
      "i: 173500 L: 326.170 norm:0.000084\n",
      "i: 174000 L: 326.163 norm:0.000084\n",
      "i: 174500 L: 326.157 norm:0.000083\n",
      "i: 175000 L: 326.150 norm:0.000083\n",
      "i: 175500 L: 326.143 norm:0.000083\n",
      "i: 176000 L: 326.137 norm:0.000083\n",
      "i: 176500 L: 326.130 norm:0.000082\n",
      "i: 177000 L: 326.124 norm:0.000082\n",
      "i: 177500 L: 326.117 norm:0.000082\n",
      "i: 178000 L: 326.111 norm:0.000082\n",
      "i: 178500 L: 326.105 norm:0.000082\n",
      "i: 179000 L: 326.098 norm:0.000081\n",
      "i: 179500 L: 326.092 norm:0.000081\n",
      "i: 180000 L: 326.086 norm:0.000081\n",
      "i: 180500 L: 326.080 norm:0.000081\n",
      "i: 181000 L: 326.073 norm:0.000080\n",
      "i: 181500 L: 326.067 norm:0.000080\n",
      "i: 182000 L: 326.061 norm:0.000080\n",
      "i: 182500 L: 326.055 norm:0.000080\n",
      "i: 183000 L: 326.049 norm:0.000080\n",
      "i: 183500 L: 326.043 norm:0.000079\n",
      "i: 184000 L: 326.037 norm:0.000079\n",
      "i: 184500 L: 326.031 norm:0.000079\n",
      "i: 185000 L: 326.025 norm:0.000079\n",
      "i: 185500 L: 326.019 norm:0.000078\n",
      "i: 186000 L: 326.013 norm:0.000078\n",
      "i: 186500 L: 326.007 norm:0.000078\n",
      "i: 187000 L: 326.001 norm:0.000078\n",
      "i: 187500 L: 325.995 norm:0.000078\n",
      "i: 188000 L: 325.989 norm:0.000077\n",
      "i: 188500 L: 325.983 norm:0.000077\n",
      "i: 189000 L: 325.978 norm:0.000077\n",
      "i: 189500 L: 325.972 norm:0.000077\n",
      "i: 190000 L: 325.966 norm:0.000077\n",
      "i: 190500 L: 325.960 norm:0.000076\n",
      "i: 191000 L: 325.955 norm:0.000076\n",
      "i: 191500 L: 325.949 norm:0.000076\n",
      "i: 192000 L: 325.943 norm:0.000076\n",
      "i: 192500 L: 325.938 norm:0.000076\n",
      "i: 193000 L: 325.932 norm:0.000075\n",
      "i: 193500 L: 325.927 norm:0.000075\n",
      "i: 194000 L: 325.921 norm:0.000075\n",
      "i: 194500 L: 325.916 norm:0.000075\n",
      "i: 195000 L: 325.910 norm:0.000075\n",
      "i: 195500 L: 325.905 norm:0.000074\n",
      "i: 196000 L: 325.899 norm:0.000074\n",
      "i: 196500 L: 325.894 norm:0.000074\n",
      "i: 197000 L: 325.889 norm:0.000074\n",
      "i: 197500 L: 325.883 norm:0.000074\n",
      "i: 198000 L: 325.878 norm:0.000073\n",
      "i: 198500 L: 325.873 norm:0.000073\n",
      "i: 199000 L: 325.867 norm:0.000073\n",
      "i: 199500 L: 325.862 norm:0.000073\n",
      "W1 matrix: \n",
      "[[ -3.94569338  -1.75329736 -10.35482363]\n",
      " [ -4.04534416  -1.77069082 -10.64246258]]\n",
      "b1 vector: \n",
      "[[0.37335154]\n",
      " [0.71720262]\n",
      " [1.6551463 ]]\n",
      "W2 matrix: \n",
      "[[ 5.25920207]\n",
      " [ 4.25734337]\n",
      " [13.56365085]]\n",
      "b2 vector: \n",
      "[[0.76618306]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_iter = 200000                        # Number of iterations\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2,3)/((2*3)**2)   # Weight matrix 1.\n",
    "b1 = np.random.randn(3,1)/((3*1)**2)   # Bias vector 1.\n",
    "W2 = np.random.randn(3,1)/((3*1)**2)   # Weight matrix 2.\n",
    "b2 = np.random.randn(1,1)/((1*1)**2)   # Bias vector 2.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, Y_train, W1, b1, W2, b2)]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # gradient descent \n",
    "    \n",
    "    gradient_W1, gradient_b1, gradient_W2, gradient_b2 = \\\n",
    "        L_prime(X_train, Y_train, W1, b1, W2, b2)\n",
    "    \n",
    "    W1_new = W1 - learning_rate * gradient_W1\n",
    "    b1_new = b1 - learning_rate * gradient_b1\n",
    "    W2_new = W2 - learning_rate * gradient_W2\n",
    "    b2_new = b2 - learning_rate * gradient_b2\n",
    "    \n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, Y_train, W1_new, b1_new, W2_new, b2_new))\n",
    "    \n",
    "    # L1-norm of weight/bias changing.\n",
    "    norm = np.abs(W1_new-W1).sum() + np.abs(b1_new-b1).sum() + \\\n",
    "           np.abs(W2_new-W2).sum() + np.abs(b2_new-b2).sum() \n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print('i: {:6d} L: {:.3f} norm:{:.6f}'.format(i, L_list[-1], norm))\n",
    "        \n",
    "    W1 = W1_new\n",
    "    b1 = b1_new\n",
    "    W2 = W2_new\n",
    "    b2 = b2_new\n",
    "    \n",
    "print ('W1 matrix: \\n' + str(W1))\n",
    "print ('b1 vector: \\n' + str(b1))\n",
    "print ('W2 matrix: \\n' + str(W2))\n",
    "print ('b2 vector: \\n' + str(b2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.losses.cosine_similarity(y_true, y_pred, axis=-1)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how simple this is using keras :)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# we specify we want to use Stochastic Gradient Descent to optimize the loss and minimize it\n",
    "# the loss we use here is known as Mean Squared Error, basically 1/N * (Y_true - Y_predicted)^2\n",
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.cosine_similarity,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# try fiddling around with other losses such as 'mae' - mean absolute error, huber loss etc. \n",
    "# Just set the loss keyword parameter to one of the below when calling model.compile\n",
    "# note that these won't all work nicely. It's always a balance of the optimizer and loss function\n",
    "# tf.keras.losses.huber\n",
    "# tf.keras.losses.mean_squared_logarithmic_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 496us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 559us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 540us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 530us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 591us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 617us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 567us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 580us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 594us/step - loss: -0.9820 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 572us/step - loss: -0.9820 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f89c3a0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
