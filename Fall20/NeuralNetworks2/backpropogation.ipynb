{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss function: L'(W1, b1, W2, b2).\n",
    "def L_prime(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L'(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the gradients: dL/dW1 (Shape: [2,3]), dL/db1 (Shape: [3,1]),\n",
    "                          dL/dW2 (Shape: [3,1]), dL/db2 (Shape: [1,1]).\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "\n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                          # Shape: [n, 3].\n",
    "    P = Y*(W2.T.dot(H.T)+b2).T                           # Shape: [n, 1].\n",
    "#     print(P.shape)\n",
    "    # Calculate the gradients: dL/dW1, dL/db1, dL/dW2, dL/db2.\n",
    "    dL_by_dW2 = H.T.dot((P-1)*Y)                            # Shape: [3,1].\n",
    "    \n",
    "#     dL_by_db2 =  (P-1).T.dot(Y)                           # Shape: [1,1].\n",
    "    dL_by_db2 = np.ones((n,1)).T.dot((P-1)*Y)\n",
    "    \n",
    "#     print(W2.shape)\n",
    "    dL_by_dH  = ((P-1)*Y).dot(W2.T)                           # Shape: [n,3].\n",
    "    dL_by_dW1  = X.T.dot(dL_by_dH*H*(1-H))                   # Shape: [2,3].\n",
    "#     print(dL_by_dW1.shape)\n",
    "    dL_by_db1  = (dL_by_dH*H*(1-H)).T.dot(np.ones((n,1)))                        # Shape: [3,1].\n",
    "#     print(dL_by_db1.shape)\n",
    "    return dL_by_dW1, dL_by_db1, dL_by_dW2, dL_by_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def L(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the loss.       Shape: Scalar.\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "#     print(X.shape)\n",
    "    \n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                             # Shape: [n, 3].\n",
    "#     print(H.shape)\n",
    "#     print(W2.T.dot(H.T).shape)\n",
    "    P = sigmoid(Y*(W2.T.dot(H.T)+b2).T)                             # Shape: [n, 1].\n",
    "    \n",
    "#     print((W2.T.dot(H.T)+b2).shape)\n",
    "#     print(P.shape)\n",
    "    # Get the loss.\n",
    "    L =    -np.sum(np.log(P))                        # Shape: Scalar.\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2) (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# lets generate some data using a function mapping from R^2 -> R^1 (2d coordinates to scalar values)\n",
    "def generate_data():\n",
    "    \n",
    "    # generates 1000 ordered data points from 0 to 1 with a bit of noise using random.uniform\n",
    "    def generate_linear_noisy():\n",
    "        return np.linspace(0, 1, num=1000) + np.random.uniform(-0.05, 0.05, (1000,))\n",
    "    \n",
    "    X_train = np.array([generate_linear_noisy(), generate_linear_noisy()]).T\n",
    "    \n",
    "    # the function modeled here is F(x, y) -> x / 2 + y / 2\n",
    "    Y_train = (X_train[:,0] * 0.5 + X_train[:,1] * 0.5).reshape(1000, 1)\n",
    "    return X_train, Y_train\n",
    "X_train, Y_train = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.28963942e-02 -4.79049931e-02]\n",
      " [-4.02860690e-02 -7.83744764e-03]\n",
      " [-4.59761583e-02  9.47865545e-04]\n",
      " ...\n",
      " [ 9.58745255e-01  9.74808924e-01]\n",
      " [ 9.77753449e-01  9.62158684e-01]\n",
      " [ 9.95670363e-01  9.53921054e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# supposed to find where loss is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:      0 L: 499.138 norm:0.055375\n",
      "i:    500 L: 406.237 norm:0.002231\n",
      "i:   1000 L: 403.019 norm:0.003711\n",
      "i:   1500 L: 396.847 norm:0.005377\n",
      "i:   2000 L: 386.841 norm:0.006411\n",
      "i:   2500 L: 374.796 norm:0.006002\n",
      "i:   3000 L: 364.492 norm:0.004493\n",
      "i:   3500 L: 357.689 norm:0.002976\n",
      "i:   4000 L: 353.621 norm:0.002239\n",
      "i:   4500 L: 351.071 norm:0.001972\n",
      "i:   5000 L: 349.263 norm:0.001783\n",
      "i:   5500 L: 347.826 norm:0.001650\n",
      "i:   6000 L: 346.602 norm:0.001552\n",
      "i:   6500 L: 345.529 norm:0.001480\n",
      "i:   7000 L: 344.573 norm:0.001409\n",
      "i:   7500 L: 343.716 norm:0.001338\n",
      "i:   8000 L: 342.943 norm:0.001269\n",
      "i:   8500 L: 342.243 norm:0.001203\n",
      "i:   9000 L: 341.606 norm:0.001138\n",
      "i:   9500 L: 341.026 norm:0.001077\n",
      "i:  10000 L: 340.496 norm:0.001018\n",
      "i:  10500 L: 340.009 norm:0.000963\n",
      "i:  11000 L: 339.562 norm:0.000912\n",
      "i:  11500 L: 339.150 norm:0.000868\n",
      "i:  12000 L: 338.769 norm:0.000831\n",
      "i:  12500 L: 338.415 norm:0.000797\n",
      "i:  13000 L: 338.087 norm:0.000764\n",
      "i:  13500 L: 337.780 norm:0.000734\n",
      "i:  14000 L: 337.494 norm:0.000705\n",
      "i:  14500 L: 337.225 norm:0.000679\n",
      "i:  15000 L: 336.973 norm:0.000654\n",
      "i:  15500 L: 336.735 norm:0.000630\n",
      "i:  16000 L: 336.510 norm:0.000608\n",
      "i:  16500 L: 336.297 norm:0.000588\n",
      "i:  17000 L: 336.095 norm:0.000570\n",
      "i:  17500 L: 335.903 norm:0.000553\n",
      "i:  18000 L: 335.720 norm:0.000538\n",
      "i:  18500 L: 335.545 norm:0.000523\n",
      "i:  19000 L: 335.378 norm:0.000508\n",
      "i:  19500 L: 335.218 norm:0.000495\n",
      "i:  20000 L: 335.065 norm:0.000482\n",
      "i:  20500 L: 334.918 norm:0.000469\n",
      "i:  21000 L: 334.776 norm:0.000457\n",
      "i:  21500 L: 334.640 norm:0.000445\n",
      "i:  22000 L: 334.509 norm:0.000434\n",
      "i:  22500 L: 334.382 norm:0.000424\n",
      "i:  23000 L: 334.260 norm:0.000413\n",
      "i:  23500 L: 334.142 norm:0.000403\n",
      "i:  24000 L: 334.027 norm:0.000394\n",
      "i:  24500 L: 333.916 norm:0.000384\n",
      "i:  25000 L: 333.809 norm:0.000375\n",
      "i:  25500 L: 333.704 norm:0.000367\n",
      "i:  26000 L: 333.603 norm:0.000359\n",
      "i:  26500 L: 333.504 norm:0.000351\n",
      "i:  27000 L: 333.408 norm:0.000344\n",
      "i:  27500 L: 333.315 norm:0.000338\n",
      "i:  28000 L: 333.224 norm:0.000333\n",
      "i:  28500 L: 333.135 norm:0.000329\n",
      "i:  29000 L: 333.049 norm:0.000324\n",
      "i:  29500 L: 332.964 norm:0.000321\n",
      "i:  30000 L: 332.882 norm:0.000319\n",
      "i:  30500 L: 332.801 norm:0.000317\n",
      "i:  31000 L: 332.722 norm:0.000316\n",
      "i:  31500 L: 332.645 norm:0.000315\n",
      "i:  32000 L: 332.569 norm:0.000314\n",
      "i:  32500 L: 332.495 norm:0.000313\n",
      "i:  33000 L: 332.422 norm:0.000312\n",
      "i:  33500 L: 332.351 norm:0.000311\n",
      "i:  34000 L: 332.281 norm:0.000310\n",
      "i:  34500 L: 332.212 norm:0.000309\n",
      "i:  35000 L: 332.144 norm:0.000308\n",
      "i:  35500 L: 332.078 norm:0.000307\n",
      "i:  36000 L: 332.013 norm:0.000307\n",
      "i:  36500 L: 331.949 norm:0.000306\n",
      "i:  37000 L: 331.885 norm:0.000306\n",
      "i:  37500 L: 331.823 norm:0.000305\n",
      "i:  38000 L: 331.762 norm:0.000304\n",
      "i:  38500 L: 331.702 norm:0.000304\n",
      "i:  39000 L: 331.643 norm:0.000304\n",
      "i:  39500 L: 331.584 norm:0.000303\n",
      "i:  40000 L: 331.527 norm:0.000303\n",
      "i:  40500 L: 331.470 norm:0.000302\n",
      "i:  41000 L: 331.414 norm:0.000302\n",
      "i:  41500 L: 331.358 norm:0.000301\n",
      "i:  42000 L: 331.304 norm:0.000301\n",
      "i:  42500 L: 331.250 norm:0.000300\n",
      "i:  43000 L: 331.197 norm:0.000299\n",
      "i:  43500 L: 331.145 norm:0.000299\n",
      "i:  44000 L: 331.093 norm:0.000298\n",
      "i:  44500 L: 331.042 norm:0.000298\n",
      "i:  45000 L: 330.992 norm:0.000297\n",
      "i:  45500 L: 330.942 norm:0.000296\n",
      "i:  46000 L: 330.893 norm:0.000295\n",
      "i:  46500 L: 330.845 norm:0.000294\n",
      "i:  47000 L: 330.797 norm:0.000293\n",
      "i:  47500 L: 330.749 norm:0.000292\n",
      "i:  48000 L: 330.703 norm:0.000291\n",
      "i:  48500 L: 330.657 norm:0.000290\n",
      "i:  49000 L: 330.611 norm:0.000289\n",
      "i:  49500 L: 330.566 norm:0.000287\n",
      "i:  50000 L: 330.522 norm:0.000286\n",
      "i:  50500 L: 330.478 norm:0.000284\n",
      "i:  51000 L: 330.435 norm:0.000283\n",
      "i:  51500 L: 330.392 norm:0.000281\n",
      "i:  52000 L: 330.350 norm:0.000279\n",
      "i:  52500 L: 330.308 norm:0.000278\n",
      "i:  53000 L: 330.267 norm:0.000276\n",
      "i:  53500 L: 330.227 norm:0.000274\n",
      "i:  54000 L: 330.186 norm:0.000272\n",
      "i:  54500 L: 330.147 norm:0.000270\n",
      "i:  55000 L: 330.108 norm:0.000268\n",
      "i:  55500 L: 330.069 norm:0.000266\n",
      "i:  56000 L: 330.031 norm:0.000264\n",
      "i:  56500 L: 329.993 norm:0.000262\n",
      "i:  57000 L: 329.956 norm:0.000260\n",
      "i:  57500 L: 329.919 norm:0.000258\n",
      "i:  58000 L: 329.883 norm:0.000255\n",
      "i:  58500 L: 329.847 norm:0.000253\n",
      "i:  59000 L: 329.812 norm:0.000251\n",
      "i:  59500 L: 329.777 norm:0.000249\n",
      "i:  60000 L: 329.742 norm:0.000246\n",
      "i:  60500 L: 329.708 norm:0.000244\n",
      "i:  61000 L: 329.674 norm:0.000242\n",
      "i:  61500 L: 329.641 norm:0.000240\n",
      "i:  62000 L: 329.608 norm:0.000237\n",
      "i:  62500 L: 329.575 norm:0.000235\n",
      "i:  63000 L: 329.543 norm:0.000233\n",
      "i:  63500 L: 329.511 norm:0.000231\n",
      "i:  64000 L: 329.480 norm:0.000228\n",
      "i:  64500 L: 329.449 norm:0.000226\n",
      "i:  65000 L: 329.418 norm:0.000224\n",
      "i:  65500 L: 329.388 norm:0.000222\n",
      "i:  66000 L: 329.357 norm:0.000220\n",
      "i:  66500 L: 329.328 norm:0.000217\n",
      "i:  67000 L: 329.298 norm:0.000215\n",
      "i:  67500 L: 329.269 norm:0.000213\n",
      "i:  68000 L: 329.240 norm:0.000211\n",
      "i:  68500 L: 329.212 norm:0.000209\n",
      "i:  69000 L: 329.183 norm:0.000207\n",
      "i:  69500 L: 329.155 norm:0.000205\n",
      "i:  70000 L: 329.128 norm:0.000203\n",
      "i:  70500 L: 329.100 norm:0.000201\n",
      "i:  71000 L: 329.073 norm:0.000199\n",
      "i:  71500 L: 329.046 norm:0.000197\n",
      "i:  72000 L: 329.020 norm:0.000196\n",
      "i:  72500 L: 328.993 norm:0.000194\n",
      "i:  73000 L: 328.967 norm:0.000193\n",
      "i:  73500 L: 328.941 norm:0.000191\n",
      "i:  74000 L: 328.916 norm:0.000190\n",
      "i:  74500 L: 328.891 norm:0.000189\n",
      "i:  75000 L: 328.865 norm:0.000187\n",
      "i:  75500 L: 328.841 norm:0.000186\n",
      "i:  76000 L: 328.816 norm:0.000184\n",
      "i:  76500 L: 328.792 norm:0.000183\n",
      "i:  77000 L: 328.767 norm:0.000182\n",
      "i:  77500 L: 328.743 norm:0.000181\n",
      "i:  78000 L: 328.720 norm:0.000179\n",
      "i:  78500 L: 328.696 norm:0.000178\n",
      "i:  79000 L: 328.673 norm:0.000177\n",
      "i:  79500 L: 328.650 norm:0.000176\n",
      "i:  80000 L: 328.627 norm:0.000174\n",
      "i:  80500 L: 328.604 norm:0.000173\n",
      "i:  81000 L: 328.581 norm:0.000172\n",
      "i:  81500 L: 328.559 norm:0.000171\n",
      "i:  82000 L: 328.537 norm:0.000170\n",
      "i:  82500 L: 328.515 norm:0.000169\n",
      "i:  83000 L: 328.493 norm:0.000168\n",
      "i:  83500 L: 328.472 norm:0.000166\n",
      "i:  84000 L: 328.450 norm:0.000165\n",
      "i:  84500 L: 328.429 norm:0.000164\n",
      "i:  85000 L: 328.408 norm:0.000163\n",
      "i:  85500 L: 328.387 norm:0.000162\n",
      "i:  86000 L: 328.367 norm:0.000161\n",
      "i:  86500 L: 328.346 norm:0.000160\n",
      "i:  87000 L: 328.326 norm:0.000159\n",
      "i:  87500 L: 328.306 norm:0.000158\n",
      "i:  88000 L: 328.286 norm:0.000157\n",
      "i:  88500 L: 328.266 norm:0.000156\n",
      "i:  89000 L: 328.246 norm:0.000155\n",
      "i:  89500 L: 328.226 norm:0.000155\n",
      "i:  90000 L: 328.207 norm:0.000154\n",
      "i:  90500 L: 328.188 norm:0.000153\n",
      "i:  91000 L: 328.169 norm:0.000152\n",
      "i:  91500 L: 328.150 norm:0.000151\n",
      "i:  92000 L: 328.131 norm:0.000150\n",
      "i:  92500 L: 328.112 norm:0.000150\n",
      "i:  93000 L: 328.094 norm:0.000149\n",
      "i:  93500 L: 328.076 norm:0.000148\n",
      "i:  94000 L: 328.057 norm:0.000148\n",
      "i:  94500 L: 328.039 norm:0.000147\n",
      "i:  95000 L: 328.021 norm:0.000147\n",
      "i:  95500 L: 328.004 norm:0.000146\n",
      "i:  96000 L: 327.986 norm:0.000145\n",
      "i:  96500 L: 327.969 norm:0.000145\n",
      "i:  97000 L: 327.951 norm:0.000144\n",
      "i:  97500 L: 327.934 norm:0.000144\n",
      "i:  98000 L: 327.917 norm:0.000143\n",
      "i:  98500 L: 327.900 norm:0.000142\n",
      "i:  99000 L: 327.883 norm:0.000142\n",
      "i:  99500 L: 327.866 norm:0.000141\n",
      "i: 100000 L: 327.850 norm:0.000141\n",
      "i: 100500 L: 327.833 norm:0.000140\n",
      "i: 101000 L: 327.817 norm:0.000140\n",
      "i: 101500 L: 327.800 norm:0.000139\n",
      "i: 102000 L: 327.784 norm:0.000139\n",
      "i: 102500 L: 327.768 norm:0.000138\n",
      "i: 103000 L: 327.752 norm:0.000138\n",
      "i: 103500 L: 327.737 norm:0.000137\n",
      "i: 104000 L: 327.721 norm:0.000137\n",
      "i: 104500 L: 327.705 norm:0.000136\n",
      "i: 105000 L: 327.690 norm:0.000135\n",
      "i: 105500 L: 327.675 norm:0.000135\n",
      "i: 106000 L: 327.659 norm:0.000134\n",
      "i: 106500 L: 327.644 norm:0.000134\n",
      "i: 107000 L: 327.629 norm:0.000134\n",
      "i: 107500 L: 327.614 norm:0.000133\n",
      "i: 108000 L: 327.599 norm:0.000133\n",
      "i: 108500 L: 327.585 norm:0.000132\n",
      "i: 109000 L: 327.570 norm:0.000132\n",
      "i: 109500 L: 327.555 norm:0.000131\n",
      "i: 110000 L: 327.541 norm:0.000131\n",
      "i: 110500 L: 327.527 norm:0.000130\n",
      "i: 111000 L: 327.513 norm:0.000130\n",
      "i: 111500 L: 327.498 norm:0.000129\n",
      "i: 112000 L: 327.484 norm:0.000129\n",
      "i: 112500 L: 327.470 norm:0.000128\n",
      "i: 113000 L: 327.457 norm:0.000128\n",
      "i: 113500 L: 327.443 norm:0.000128\n",
      "i: 114000 L: 327.429 norm:0.000127\n",
      "i: 114500 L: 327.416 norm:0.000127\n",
      "i: 115000 L: 327.402 norm:0.000126\n",
      "i: 115500 L: 327.389 norm:0.000126\n",
      "i: 116000 L: 327.375 norm:0.000125\n",
      "i: 116500 L: 327.362 norm:0.000125\n",
      "i: 117000 L: 327.349 norm:0.000125\n",
      "i: 117500 L: 327.336 norm:0.000124\n",
      "i: 118000 L: 327.323 norm:0.000124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 118500 L: 327.310 norm:0.000123\n",
      "i: 119000 L: 327.297 norm:0.000123\n",
      "i: 119500 L: 327.285 norm:0.000123\n",
      "i: 120000 L: 327.272 norm:0.000122\n",
      "i: 120500 L: 327.259 norm:0.000122\n",
      "i: 121000 L: 327.247 norm:0.000121\n",
      "i: 121500 L: 327.234 norm:0.000121\n",
      "i: 122000 L: 327.222 norm:0.000121\n",
      "i: 122500 L: 327.210 norm:0.000120\n",
      "i: 123000 L: 327.198 norm:0.000120\n",
      "i: 123500 L: 327.186 norm:0.000119\n",
      "i: 124000 L: 327.174 norm:0.000119\n",
      "i: 124500 L: 327.162 norm:0.000119\n",
      "i: 125000 L: 327.150 norm:0.000118\n",
      "i: 125500 L: 327.138 norm:0.000118\n",
      "i: 126000 L: 327.126 norm:0.000117\n",
      "i: 126500 L: 327.115 norm:0.000117\n",
      "i: 127000 L: 327.103 norm:0.000117\n",
      "i: 127500 L: 327.091 norm:0.000116\n",
      "i: 128000 L: 327.080 norm:0.000116\n",
      "i: 128500 L: 327.069 norm:0.000116\n",
      "i: 129000 L: 327.057 norm:0.000115\n",
      "i: 129500 L: 327.046 norm:0.000115\n",
      "i: 130000 L: 327.035 norm:0.000114\n",
      "i: 130500 L: 327.024 norm:0.000114\n",
      "i: 131000 L: 327.013 norm:0.000114\n",
      "i: 131500 L: 327.002 norm:0.000113\n",
      "i: 132000 L: 326.991 norm:0.000113\n",
      "i: 132500 L: 326.980 norm:0.000113\n",
      "i: 133000 L: 326.969 norm:0.000112\n",
      "i: 133500 L: 326.958 norm:0.000112\n",
      "i: 134000 L: 326.947 norm:0.000112\n",
      "i: 134500 L: 326.937 norm:0.000111\n",
      "i: 135000 L: 326.926 norm:0.000111\n",
      "i: 135500 L: 326.916 norm:0.000111\n",
      "i: 136000 L: 326.905 norm:0.000110\n",
      "i: 136500 L: 326.895 norm:0.000110\n",
      "i: 137000 L: 326.884 norm:0.000110\n",
      "i: 137500 L: 326.874 norm:0.000109\n",
      "i: 138000 L: 326.864 norm:0.000109\n",
      "i: 138500 L: 326.854 norm:0.000109\n",
      "i: 139000 L: 326.844 norm:0.000108\n",
      "i: 139500 L: 326.834 norm:0.000108\n",
      "i: 140000 L: 326.824 norm:0.000108\n",
      "i: 140500 L: 326.814 norm:0.000107\n",
      "i: 141000 L: 326.804 norm:0.000107\n",
      "i: 141500 L: 326.794 norm:0.000107\n",
      "i: 142000 L: 326.784 norm:0.000106\n",
      "i: 142500 L: 326.774 norm:0.000106\n",
      "i: 143000 L: 326.765 norm:0.000106\n",
      "i: 143500 L: 326.755 norm:0.000105\n",
      "i: 144000 L: 326.746 norm:0.000105\n",
      "i: 144500 L: 326.736 norm:0.000105\n",
      "i: 145000 L: 326.726 norm:0.000104\n",
      "i: 145500 L: 326.717 norm:0.000104\n",
      "i: 146000 L: 326.708 norm:0.000104\n",
      "i: 146500 L: 326.698 norm:0.000104\n",
      "i: 147000 L: 326.689 norm:0.000103\n",
      "i: 147500 L: 326.680 norm:0.000103\n",
      "i: 148000 L: 326.671 norm:0.000103\n",
      "i: 148500 L: 326.661 norm:0.000102\n",
      "i: 149000 L: 326.652 norm:0.000102\n",
      "i: 149500 L: 326.643 norm:0.000102\n",
      "i: 150000 L: 326.634 norm:0.000102\n",
      "i: 150500 L: 326.625 norm:0.000101\n",
      "i: 151000 L: 326.616 norm:0.000101\n",
      "i: 151500 L: 326.608 norm:0.000101\n",
      "i: 152000 L: 326.599 norm:0.000100\n",
      "i: 152500 L: 326.590 norm:0.000100\n",
      "i: 153000 L: 326.581 norm:0.000100\n",
      "i: 153500 L: 326.573 norm:0.000100\n",
      "i: 154000 L: 326.564 norm:0.000099\n",
      "i: 154500 L: 326.555 norm:0.000099\n",
      "i: 155000 L: 326.547 norm:0.000099\n",
      "i: 155500 L: 326.538 norm:0.000098\n",
      "i: 156000 L: 326.530 norm:0.000098\n",
      "i: 156500 L: 326.521 norm:0.000098\n",
      "i: 157000 L: 326.513 norm:0.000098\n",
      "i: 157500 L: 326.504 norm:0.000097\n",
      "i: 158000 L: 326.496 norm:0.000097\n",
      "i: 158500 L: 326.488 norm:0.000097\n",
      "i: 159000 L: 326.480 norm:0.000097\n",
      "i: 159500 L: 326.471 norm:0.000096\n",
      "i: 160000 L: 326.463 norm:0.000096\n",
      "i: 160500 L: 326.455 norm:0.000096\n",
      "i: 161000 L: 326.447 norm:0.000095\n",
      "i: 161500 L: 326.439 norm:0.000095\n",
      "i: 162000 L: 326.431 norm:0.000095\n",
      "i: 162500 L: 326.423 norm:0.000095\n",
      "i: 163000 L: 326.415 norm:0.000094\n",
      "i: 163500 L: 326.407 norm:0.000094\n",
      "i: 164000 L: 326.399 norm:0.000094\n",
      "i: 164500 L: 326.392 norm:0.000094\n",
      "i: 165000 L: 326.384 norm:0.000093\n",
      "i: 165500 L: 326.376 norm:0.000093\n",
      "i: 166000 L: 326.368 norm:0.000093\n",
      "i: 166500 L: 326.361 norm:0.000093\n",
      "i: 167000 L: 326.353 norm:0.000092\n",
      "i: 167500 L: 326.345 norm:0.000092\n",
      "i: 168000 L: 326.338 norm:0.000092\n",
      "i: 168500 L: 326.330 norm:0.000092\n",
      "i: 169000 L: 326.323 norm:0.000091\n",
      "i: 169500 L: 326.315 norm:0.000091\n",
      "i: 170000 L: 326.308 norm:0.000091\n",
      "i: 170500 L: 326.300 norm:0.000091\n",
      "i: 171000 L: 326.293 norm:0.000090\n",
      "i: 171500 L: 326.286 norm:0.000090\n",
      "i: 172000 L: 326.278 norm:0.000090\n",
      "i: 172500 L: 326.271 norm:0.000090\n",
      "i: 173000 L: 326.264 norm:0.000090\n",
      "i: 173500 L: 326.257 norm:0.000089\n",
      "i: 174000 L: 326.249 norm:0.000089\n",
      "i: 174500 L: 326.242 norm:0.000089\n",
      "i: 175000 L: 326.235 norm:0.000089\n",
      "i: 175500 L: 326.228 norm:0.000088\n",
      "i: 176000 L: 326.221 norm:0.000088\n",
      "i: 176500 L: 326.214 norm:0.000088\n",
      "i: 177000 L: 326.207 norm:0.000088\n",
      "i: 177500 L: 326.200 norm:0.000087\n",
      "i: 178000 L: 326.193 norm:0.000087\n",
      "i: 178500 L: 326.186 norm:0.000087\n",
      "i: 179000 L: 326.179 norm:0.000087\n",
      "i: 179500 L: 326.173 norm:0.000087\n",
      "i: 180000 L: 326.166 norm:0.000086\n",
      "i: 180500 L: 326.159 norm:0.000086\n",
      "i: 181000 L: 326.152 norm:0.000086\n",
      "i: 181500 L: 326.145 norm:0.000086\n",
      "i: 182000 L: 326.139 norm:0.000086\n",
      "i: 182500 L: 326.132 norm:0.000085\n",
      "i: 183000 L: 326.125 norm:0.000085\n",
      "i: 183500 L: 326.119 norm:0.000085\n",
      "i: 184000 L: 326.112 norm:0.000085\n",
      "i: 184500 L: 326.106 norm:0.000084\n",
      "i: 185000 L: 326.099 norm:0.000084\n",
      "i: 185500 L: 326.093 norm:0.000084\n",
      "i: 186000 L: 326.086 norm:0.000084\n",
      "i: 186500 L: 326.080 norm:0.000084\n",
      "i: 187000 L: 326.073 norm:0.000083\n",
      "i: 187500 L: 326.067 norm:0.000083\n",
      "i: 188000 L: 326.061 norm:0.000083\n",
      "i: 188500 L: 326.054 norm:0.000083\n",
      "i: 189000 L: 326.048 norm:0.000083\n",
      "i: 189500 L: 326.042 norm:0.000082\n",
      "i: 190000 L: 326.035 norm:0.000082\n",
      "i: 190500 L: 326.029 norm:0.000082\n",
      "i: 191000 L: 326.023 norm:0.000082\n",
      "i: 191500 L: 326.017 norm:0.000082\n",
      "i: 192000 L: 326.010 norm:0.000081\n",
      "i: 192500 L: 326.004 norm:0.000081\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_iter = 200000                        # Number of iterations\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2,3)/((2*3)**2)   # Weight matrix 1.\n",
    "b1 = np.random.randn(3,1)/((3*1)**2)   # Bias vector 1.\n",
    "W2 = np.random.randn(3,1)/((3*1)**2)   # Weight matrix 2.\n",
    "b2 = np.random.randn(1,1)/((1*1)**2)   # Bias vector 2.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, Y_train, W1, b1, W2, b2)]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # gradient descent \n",
    "    \n",
    "    gradient_W1, gradient_b1, gradient_W2, gradient_b2 = \\\n",
    "        L_prime(X_train, Y_train, W1, b1, W2, b2)\n",
    "    \n",
    "    W1_new = W1 - learning_rate * gradient_W1\n",
    "    b1_new = b1 - learning_rate * gradient_b1\n",
    "    W2_new = W2 - learning_rate * gradient_W2\n",
    "    b2_new = b2 - learning_rate * gradient_b2\n",
    "    \n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, Y_train, W1_new, b1_new, W2_new, b2_new))\n",
    "    \n",
    "    # L1-norm of weight/bias changing.\n",
    "    norm = np.abs(W1_new-W1).sum() + np.abs(b1_new-b1).sum() + \\\n",
    "           np.abs(W2_new-W2).sum() + np.abs(b2_new-b2).sum() \n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print('i: {:6d} L: {:.3f} norm:{:.6f}'.format(i, L_list[-1], norm))\n",
    "        \n",
    "    W1 = W1_new\n",
    "    b1 = b1_new\n",
    "    W2 = W2_new\n",
    "    b2 = b2_new\n",
    "    \n",
    "print ('W1 matrix: \\n' + str(W1))\n",
    "print ('b1 vector: \\n' + str(b1))\n",
    "print ('W2 matrix: \\n' + str(W2))\n",
    "print ('b2 vector: \\n' + str(b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropogation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer=tf.train.GradientDescentOptimizer(0.01).minimize(f_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
