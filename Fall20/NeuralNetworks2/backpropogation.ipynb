{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss function: L'(W1, b1, W2, b2).\n",
    "def L_prime(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L'(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the gradients: dL/dW1 (Shape: [2,3]), dL/db1 (Shape: [3,1]),\n",
    "                          dL/dW2 (Shape: [3,1]), dL/db2 (Shape: [1,1]).\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "\n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                          # Shape: [n, 3].\n",
    "    P = Y*(W2.T.dot(H.T)+b2).T                           # Shape: [n, 1].\n",
    "#     print(P.shape)\n",
    "    # Calculate the gradients: dL/dW1, dL/db1, dL/dW2, dL/db2.\n",
    "    dL_by_dW2 = H.T.dot((P-1)*Y)                            # Shape: [3,1].\n",
    "    \n",
    "#     dL_by_db2 =  (P-1).T.dot(Y)                           # Shape: [1,1].\n",
    "    dL_by_db2 = np.ones((n,1)).T.dot((P-1)*Y)\n",
    "    \n",
    "#     print(W2.shape)\n",
    "    dL_by_dH  = ((P-1)*Y).dot(W2.T)                           # Shape: [n,3].\n",
    "    dL_by_dW1  = X.T.dot(dL_by_dH*H*(1-H))                   # Shape: [2,3].\n",
    "#     print(dL_by_dW1.shape)\n",
    "    dL_by_db1  = (dL_by_dH*H*(1-H)).T.dot(np.ones((n,1)))                        # Shape: [3,1].\n",
    "#     print(dL_by_db1.shape)\n",
    "    return dL_by_dW1, dL_by_db1, dL_by_dW2, dL_by_db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def L(X, Y, W1, b1, W2, b2):\n",
    "    \"\"\" L(W,b) function. \n",
    "    X:  Feature matrix.    Shape: [n,2].\n",
    "    Y:  Label vector.      Shape: [n,1].\n",
    "    W1: Weight matrix W1.  Shape: [2,3].\n",
    "    b1: Bias vector b1.    Shape: [3,1].\n",
    "    W2: Weight matrix W2.  Shape: [3,1].\n",
    "    b2: Bias vector b2.    Shape: [1,1].\n",
    "    Return the loss.       Shape: Scalar.\n",
    "    \"\"\"\n",
    "    # Get dimensions.\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate feed-forward values.\n",
    "#     print(X.shape)\n",
    "    \n",
    "    H = sigmoid(W1.T.dot(X.T) + b1).T                             # Shape: [n, 3].\n",
    "#     print(H.shape)\n",
    "#     print(W2.T.dot(H.T).shape)\n",
    "    P = sigmoid(Y*(W2.T.dot(H.T)+b2).T)                             # Shape: [n, 1].\n",
    "    \n",
    "#     print((W2.T.dot(H.T)+b2).shape)\n",
    "#     print(P.shape)\n",
    "    # Get the loss.\n",
    "    L =    -np.sum(np.log(P))                        # Shape: Scalar.\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate some data using a function mapping from R^2 -> R^1 (2d coordinates to scalar values)\n",
    "def generate_data():\n",
    "    \n",
    "    # generates 1000 ordered data points from 0 to 1 with a bit of noise using random.uniform\n",
    "    def generate_linear_noisy():\n",
    "        return np.linspace(0, 1, num=1000) + np.random.uniform(-0.05, 0.05, (1000,))\n",
    "    \n",
    "    X_train = np.array([generate_linear_noisy(), generate_linear_noisy()]).T\n",
    "    \n",
    "    # the function modeled here is F(x, y) -> x / 2 + y / 2\n",
    "    Y_train = (X_train[:,0] * 0.5 + X_train[:,1] * 0.5).reshape(1000, 1)\n",
    "    return X_train, Y_train\n",
    "X_train, Y_train = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04739148  0.03284548]\n",
      " [-0.01272119 -0.0220772 ]\n",
      " [ 0.0338837  -0.03660984]\n",
      " ...\n",
      " [ 1.04537137  1.00214967]\n",
      " [ 1.01571349  0.97313234]\n",
      " [ 0.97557795  0.9906448 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "# supposed to find where loss is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:      0 L: 498.674 norm:0.055257\n",
      "i:    500 L: 406.410 norm:0.002239\n",
      "i:   1000 L: 403.164 norm:0.003733\n",
      "i:   1500 L: 396.925 norm:0.005403\n",
      "i:   2000 L: 386.843 norm:0.006417\n",
      "i:   2500 L: 374.793 norm:0.005971\n",
      "i:   3000 L: 364.575 norm:0.004444\n",
      "i:   3500 L: 357.869 norm:0.002940\n",
      "i:   4000 L: 353.861 norm:0.002231\n",
      "i:   4500 L: 351.339 norm:0.001968\n",
      "i:   5000 L: 349.543 norm:0.001782\n",
      "i:   5500 L: 348.109 norm:0.001648\n",
      "i:   6000 L: 346.888 norm:0.001552\n",
      "i:   6500 L: 345.816 norm:0.001480\n",
      "i:   7000 L: 344.861 norm:0.001408\n",
      "i:   7500 L: 344.005 norm:0.001337\n",
      "i:   8000 L: 343.233 norm:0.001268\n",
      "i:   8500 L: 342.533 norm:0.001202\n",
      "i:   9000 L: 341.897 norm:0.001137\n",
      "i:   9500 L: 341.317 norm:0.001076\n",
      "i:  10000 L: 340.786 norm:0.001018\n",
      "i:  10500 L: 340.299 norm:0.000964\n",
      "i:  11000 L: 339.850 norm:0.000914\n",
      "i:  11500 L: 339.436 norm:0.000873\n",
      "i:  12000 L: 339.053 norm:0.000836\n",
      "i:  12500 L: 338.697 norm:0.000802\n",
      "i:  13000 L: 338.366 norm:0.000770\n",
      "i:  13500 L: 338.057 norm:0.000740\n",
      "i:  14000 L: 337.767 norm:0.000712\n",
      "i:  14500 L: 337.495 norm:0.000686\n",
      "i:  15000 L: 337.239 norm:0.000661\n",
      "i:  15500 L: 336.997 norm:0.000638\n",
      "i:  16000 L: 336.768 norm:0.000616\n",
      "i:  16500 L: 336.551 norm:0.000596\n",
      "i:  17000 L: 336.345 norm:0.000578\n",
      "i:  17500 L: 336.148 norm:0.000561\n",
      "i:  18000 L: 335.961 norm:0.000546\n",
      "i:  18500 L: 335.782 norm:0.000531\n",
      "i:  19000 L: 335.610 norm:0.000517\n",
      "i:  19500 L: 335.446 norm:0.000503\n",
      "i:  20000 L: 335.288 norm:0.000490\n",
      "i:  20500 L: 335.137 norm:0.000477\n",
      "i:  21000 L: 334.991 norm:0.000465\n",
      "i:  21500 L: 334.850 norm:0.000453\n",
      "i:  22000 L: 334.714 norm:0.000442\n",
      "i:  22500 L: 334.583 norm:0.000431\n",
      "i:  23000 L: 334.456 norm:0.000421\n",
      "i:  23500 L: 334.333 norm:0.000410\n",
      "i:  24000 L: 334.214 norm:0.000401\n",
      "i:  24500 L: 334.099 norm:0.000391\n",
      "i:  25000 L: 333.987 norm:0.000382\n",
      "i:  25500 L: 333.879 norm:0.000374\n",
      "i:  26000 L: 333.773 norm:0.000366\n",
      "i:  26500 L: 333.670 norm:0.000359\n",
      "i:  27000 L: 333.570 norm:0.000353\n",
      "i:  27500 L: 333.472 norm:0.000348\n",
      "i:  28000 L: 333.377 norm:0.000343\n",
      "i:  28500 L: 333.284 norm:0.000338\n",
      "i:  29000 L: 333.194 norm:0.000335\n",
      "i:  29500 L: 333.105 norm:0.000332\n",
      "i:  30000 L: 333.018 norm:0.000330\n",
      "i:  30500 L: 332.934 norm:0.000329\n",
      "i:  31000 L: 332.851 norm:0.000328\n",
      "i:  31500 L: 332.769 norm:0.000326\n",
      "i:  32000 L: 332.690 norm:0.000325\n",
      "i:  32500 L: 332.612 norm:0.000324\n",
      "i:  33000 L: 332.535 norm:0.000323\n",
      "i:  33500 L: 332.460 norm:0.000322\n",
      "i:  34000 L: 332.386 norm:0.000322\n",
      "i:  34500 L: 332.313 norm:0.000321\n",
      "i:  35000 L: 332.242 norm:0.000320\n",
      "i:  35500 L: 332.172 norm:0.000319\n",
      "i:  36000 L: 332.103 norm:0.000319\n",
      "i:  36500 L: 332.035 norm:0.000318\n",
      "i:  37000 L: 331.968 norm:0.000318\n",
      "i:  37500 L: 331.902 norm:0.000317\n",
      "i:  38000 L: 331.838 norm:0.000317\n",
      "i:  38500 L: 331.774 norm:0.000316\n",
      "i:  39000 L: 331.711 norm:0.000316\n",
      "i:  39500 L: 331.649 norm:0.000315\n",
      "i:  40000 L: 331.588 norm:0.000315\n",
      "i:  40500 L: 331.528 norm:0.000314\n",
      "i:  41000 L: 331.468 norm:0.000314\n",
      "i:  41500 L: 331.410 norm:0.000313\n",
      "i:  42000 L: 331.352 norm:0.000313\n",
      "i:  42500 L: 331.295 norm:0.000312\n",
      "i:  43000 L: 331.238 norm:0.000311\n",
      "i:  43500 L: 331.183 norm:0.000311\n",
      "i:  44000 L: 331.128 norm:0.000310\n",
      "i:  44500 L: 331.074 norm:0.000309\n",
      "i:  45000 L: 331.020 norm:0.000308\n",
      "i:  45500 L: 330.967 norm:0.000307\n",
      "i:  46000 L: 330.915 norm:0.000306\n",
      "i:  46500 L: 330.864 norm:0.000305\n",
      "i:  47000 L: 330.813 norm:0.000304\n",
      "i:  47500 L: 330.763 norm:0.000302\n",
      "i:  48000 L: 330.713 norm:0.000301\n",
      "i:  48500 L: 330.664 norm:0.000300\n",
      "i:  49000 L: 330.616 norm:0.000298\n",
      "i:  49500 L: 330.568 norm:0.000296\n",
      "i:  50000 L: 330.521 norm:0.000295\n",
      "i:  50500 L: 330.475 norm:0.000293\n",
      "i:  51000 L: 330.429 norm:0.000291\n",
      "i:  51500 L: 330.384 norm:0.000289\n",
      "i:  52000 L: 330.339 norm:0.000287\n",
      "i:  52500 L: 330.295 norm:0.000285\n",
      "i:  53000 L: 330.251 norm:0.000283\n",
      "i:  53500 L: 330.208 norm:0.000281\n",
      "i:  54000 L: 330.166 norm:0.000279\n",
      "i:  54500 L: 330.124 norm:0.000277\n",
      "i:  55000 L: 330.083 norm:0.000274\n",
      "i:  55500 L: 330.042 norm:0.000272\n",
      "i:  56000 L: 330.001 norm:0.000270\n",
      "i:  56500 L: 329.962 norm:0.000267\n",
      "i:  57000 L: 329.922 norm:0.000265\n",
      "i:  57500 L: 329.883 norm:0.000262\n",
      "i:  58000 L: 329.845 norm:0.000260\n",
      "i:  58500 L: 329.807 norm:0.000257\n",
      "i:  59000 L: 329.770 norm:0.000255\n",
      "i:  59500 L: 329.733 norm:0.000253\n",
      "i:  60000 L: 329.696 norm:0.000250\n",
      "i:  60500 L: 329.660 norm:0.000248\n",
      "i:  61000 L: 329.625 norm:0.000245\n",
      "i:  61500 L: 329.589 norm:0.000243\n",
      "i:  62000 L: 329.555 norm:0.000240\n",
      "i:  62500 L: 329.520 norm:0.000238\n",
      "i:  63000 L: 329.486 norm:0.000235\n",
      "i:  63500 L: 329.453 norm:0.000233\n",
      "i:  64000 L: 329.419 norm:0.000231\n",
      "i:  64500 L: 329.387 norm:0.000228\n",
      "i:  65000 L: 329.354 norm:0.000226\n",
      "i:  65500 L: 329.322 norm:0.000224\n",
      "i:  66000 L: 329.290 norm:0.000221\n",
      "i:  66500 L: 329.259 norm:0.000219\n",
      "i:  67000 L: 329.228 norm:0.000217\n",
      "i:  67500 L: 329.197 norm:0.000215\n",
      "i:  68000 L: 329.167 norm:0.000212\n",
      "i:  68500 L: 329.137 norm:0.000210\n",
      "i:  69000 L: 329.107 norm:0.000208\n",
      "i:  69500 L: 329.078 norm:0.000206\n",
      "i:  70000 L: 329.048 norm:0.000204\n",
      "i:  70500 L: 329.020 norm:0.000202\n",
      "i:  71000 L: 328.991 norm:0.000200\n",
      "i:  71500 L: 328.963 norm:0.000199\n",
      "i:  72000 L: 328.935 norm:0.000197\n",
      "i:  72500 L: 328.907 norm:0.000196\n",
      "i:  73000 L: 328.880 norm:0.000194\n",
      "i:  73500 L: 328.853 norm:0.000193\n",
      "i:  74000 L: 328.826 norm:0.000191\n",
      "i:  74500 L: 328.799 norm:0.000190\n",
      "i:  75000 L: 328.773 norm:0.000188\n",
      "i:  75500 L: 328.747 norm:0.000187\n",
      "i:  76000 L: 328.721 norm:0.000185\n",
      "i:  76500 L: 328.696 norm:0.000184\n",
      "i:  77000 L: 328.670 norm:0.000183\n",
      "i:  77500 L: 328.645 norm:0.000181\n",
      "i:  78000 L: 328.621 norm:0.000180\n",
      "i:  78500 L: 328.596 norm:0.000178\n",
      "i:  79000 L: 328.572 norm:0.000177\n",
      "i:  79500 L: 328.548 norm:0.000176\n",
      "i:  80000 L: 328.524 norm:0.000175\n",
      "i:  80500 L: 328.500 norm:0.000173\n",
      "i:  81000 L: 328.477 norm:0.000172\n",
      "i:  81500 L: 328.453 norm:0.000171\n",
      "i:  82000 L: 328.430 norm:0.000170\n",
      "i:  82500 L: 328.407 norm:0.000168\n",
      "i:  83000 L: 328.385 norm:0.000167\n",
      "i:  83500 L: 328.362 norm:0.000166\n",
      "i:  84000 L: 328.340 norm:0.000165\n",
      "i:  84500 L: 328.318 norm:0.000164\n",
      "i:  85000 L: 328.296 norm:0.000163\n",
      "i:  85500 L: 328.275 norm:0.000162\n",
      "i:  86000 L: 328.253 norm:0.000161\n",
      "i:  86500 L: 328.232 norm:0.000160\n",
      "i:  87000 L: 328.211 norm:0.000159\n",
      "i:  87500 L: 328.190 norm:0.000157\n",
      "i:  88000 L: 328.170 norm:0.000156\n",
      "i:  88500 L: 328.149 norm:0.000155\n",
      "i:  89000 L: 328.129 norm:0.000154\n",
      "i:  89500 L: 328.109 norm:0.000154\n",
      "i:  90000 L: 328.089 norm:0.000153\n",
      "i:  90500 L: 328.069 norm:0.000152\n",
      "i:  91000 L: 328.049 norm:0.000151\n",
      "i:  91500 L: 328.030 norm:0.000150\n",
      "i:  92000 L: 328.011 norm:0.000150\n",
      "i:  92500 L: 327.991 norm:0.000149\n",
      "i:  93000 L: 327.973 norm:0.000148\n",
      "i:  93500 L: 327.954 norm:0.000148\n",
      "i:  94000 L: 327.935 norm:0.000147\n",
      "i:  94500 L: 327.917 norm:0.000146\n",
      "i:  95000 L: 327.898 norm:0.000146\n",
      "i:  95500 L: 327.880 norm:0.000145\n",
      "i:  96000 L: 327.862 norm:0.000144\n",
      "i:  96500 L: 327.844 norm:0.000144\n",
      "i:  97000 L: 327.826 norm:0.000143\n",
      "i:  97500 L: 327.809 norm:0.000142\n",
      "i:  98000 L: 327.791 norm:0.000142\n",
      "i:  98500 L: 327.774 norm:0.000141\n",
      "i:  99000 L: 327.757 norm:0.000141\n",
      "i:  99500 L: 327.740 norm:0.000140\n",
      "i: 100000 L: 327.723 norm:0.000139\n",
      "i: 100500 L: 327.706 norm:0.000139\n",
      "i: 101000 L: 327.690 norm:0.000138\n",
      "i: 101500 L: 327.673 norm:0.000137\n",
      "i: 102000 L: 327.657 norm:0.000137\n",
      "i: 102500 L: 327.641 norm:0.000136\n",
      "i: 103000 L: 327.625 norm:0.000136\n",
      "i: 103500 L: 327.609 norm:0.000135\n",
      "i: 104000 L: 327.593 norm:0.000135\n",
      "i: 104500 L: 327.577 norm:0.000134\n",
      "i: 105000 L: 327.561 norm:0.000134\n",
      "i: 105500 L: 327.546 norm:0.000133\n",
      "i: 106000 L: 327.531 norm:0.000132\n",
      "i: 106500 L: 327.515 norm:0.000132\n",
      "i: 107000 L: 327.500 norm:0.000131\n",
      "i: 107500 L: 327.485 norm:0.000131\n",
      "i: 108000 L: 327.470 norm:0.000130\n",
      "i: 108500 L: 327.456 norm:0.000130\n",
      "i: 109000 L: 327.441 norm:0.000129\n",
      "i: 109500 L: 327.426 norm:0.000129\n",
      "i: 110000 L: 327.412 norm:0.000128\n",
      "i: 110500 L: 327.397 norm:0.000128\n",
      "i: 111000 L: 327.383 norm:0.000127\n",
      "i: 111500 L: 327.369 norm:0.000127\n",
      "i: 112000 L: 327.355 norm:0.000126\n",
      "i: 112500 L: 327.341 norm:0.000126\n",
      "i: 113000 L: 327.327 norm:0.000125\n",
      "i: 113500 L: 327.314 norm:0.000125\n",
      "i: 114000 L: 327.300 norm:0.000124\n",
      "i: 114500 L: 327.286 norm:0.000124\n",
      "i: 115000 L: 327.273 norm:0.000123\n",
      "i: 115500 L: 327.260 norm:0.000123\n",
      "i: 116000 L: 327.246 norm:0.000123\n",
      "i: 116500 L: 327.233 norm:0.000122\n",
      "i: 117000 L: 327.220 norm:0.000122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 117500 L: 327.207 norm:0.000121\n",
      "i: 118000 L: 327.194 norm:0.000121\n",
      "i: 118500 L: 327.182 norm:0.000120\n",
      "i: 119000 L: 327.169 norm:0.000120\n",
      "i: 119500 L: 327.156 norm:0.000119\n",
      "i: 120000 L: 327.144 norm:0.000119\n",
      "i: 120500 L: 327.132 norm:0.000118\n",
      "i: 121000 L: 327.119 norm:0.000118\n",
      "i: 121500 L: 327.107 norm:0.000118\n",
      "i: 122000 L: 327.095 norm:0.000117\n",
      "i: 122500 L: 327.083 norm:0.000117\n",
      "i: 123000 L: 327.071 norm:0.000116\n",
      "i: 123500 L: 327.059 norm:0.000116\n",
      "i: 124000 L: 327.047 norm:0.000115\n",
      "i: 124500 L: 327.035 norm:0.000115\n",
      "i: 125000 L: 327.024 norm:0.000115\n",
      "i: 125500 L: 327.012 norm:0.000114\n",
      "i: 126000 L: 327.000 norm:0.000114\n",
      "i: 126500 L: 326.989 norm:0.000113\n",
      "i: 127000 L: 326.978 norm:0.000113\n",
      "i: 127500 L: 326.966 norm:0.000113\n",
      "i: 128000 L: 326.955 norm:0.000112\n",
      "i: 128500 L: 326.944 norm:0.000112\n",
      "i: 129000 L: 326.933 norm:0.000111\n",
      "i: 129500 L: 326.922 norm:0.000111\n",
      "i: 130000 L: 326.911 norm:0.000111\n",
      "i: 130500 L: 326.900 norm:0.000110\n",
      "i: 131000 L: 326.889 norm:0.000110\n",
      "i: 131500 L: 326.879 norm:0.000109\n",
      "i: 132000 L: 326.868 norm:0.000109\n",
      "i: 132500 L: 326.857 norm:0.000109\n",
      "i: 133000 L: 326.847 norm:0.000108\n",
      "i: 133500 L: 326.836 norm:0.000108\n",
      "i: 134000 L: 326.826 norm:0.000108\n",
      "i: 134500 L: 326.816 norm:0.000107\n",
      "i: 135000 L: 326.805 norm:0.000107\n",
      "i: 135500 L: 326.795 norm:0.000106\n",
      "i: 136000 L: 326.785 norm:0.000106\n",
      "i: 136500 L: 326.775 norm:0.000106\n",
      "i: 137000 L: 326.765 norm:0.000105\n",
      "i: 137500 L: 326.755 norm:0.000105\n",
      "i: 138000 L: 326.745 norm:0.000105\n",
      "i: 138500 L: 326.735 norm:0.000104\n",
      "i: 139000 L: 326.726 norm:0.000104\n",
      "i: 139500 L: 326.716 norm:0.000104\n",
      "i: 140000 L: 326.706 norm:0.000103\n",
      "i: 140500 L: 326.697 norm:0.000103\n",
      "i: 141000 L: 326.687 norm:0.000103\n",
      "i: 141500 L: 326.678 norm:0.000102\n",
      "i: 142000 L: 326.668 norm:0.000102\n",
      "i: 142500 L: 326.659 norm:0.000102\n",
      "i: 143000 L: 326.650 norm:0.000101\n",
      "i: 143500 L: 326.640 norm:0.000101\n",
      "i: 144000 L: 326.631 norm:0.000101\n",
      "i: 144500 L: 326.622 norm:0.000100\n",
      "i: 145000 L: 326.613 norm:0.000100\n",
      "i: 145500 L: 326.604 norm:0.000100\n",
      "i: 146000 L: 326.595 norm:0.000099\n",
      "i: 146500 L: 326.586 norm:0.000099\n",
      "i: 147000 L: 326.577 norm:0.000099\n",
      "i: 147500 L: 326.568 norm:0.000098\n",
      "i: 148000 L: 326.559 norm:0.000098\n",
      "i: 148500 L: 326.551 norm:0.000098\n",
      "i: 149000 L: 326.542 norm:0.000097\n",
      "i: 149500 L: 326.533 norm:0.000097\n",
      "i: 150000 L: 326.525 norm:0.000097\n",
      "i: 150500 L: 326.516 norm:0.000096\n",
      "i: 151000 L: 326.508 norm:0.000096\n",
      "i: 151500 L: 326.499 norm:0.000096\n",
      "i: 152000 L: 326.491 norm:0.000095\n",
      "i: 152500 L: 326.483 norm:0.000095\n",
      "i: 153000 L: 326.474 norm:0.000095\n",
      "i: 153500 L: 326.466 norm:0.000095\n",
      "i: 154000 L: 326.458 norm:0.000094\n",
      "i: 154500 L: 326.450 norm:0.000094\n",
      "i: 155000 L: 326.442 norm:0.000094\n",
      "i: 155500 L: 326.434 norm:0.000093\n",
      "i: 156000 L: 326.426 norm:0.000093\n",
      "i: 156500 L: 326.418 norm:0.000093\n",
      "i: 157000 L: 326.410 norm:0.000093\n",
      "i: 157500 L: 326.402 norm:0.000092\n",
      "i: 158000 L: 326.394 norm:0.000092\n",
      "i: 158500 L: 326.386 norm:0.000092\n",
      "i: 159000 L: 326.378 norm:0.000091\n",
      "i: 159500 L: 326.371 norm:0.000091\n",
      "i: 160000 L: 326.363 norm:0.000091\n",
      "i: 160500 L: 326.355 norm:0.000091\n",
      "i: 161000 L: 326.348 norm:0.000090\n",
      "i: 161500 L: 326.340 norm:0.000090\n",
      "i: 162000 L: 326.333 norm:0.000090\n",
      "i: 162500 L: 326.325 norm:0.000089\n",
      "i: 163000 L: 326.318 norm:0.000089\n",
      "i: 163500 L: 326.310 norm:0.000089\n",
      "i: 164000 L: 326.303 norm:0.000089\n",
      "i: 164500 L: 326.296 norm:0.000088\n",
      "i: 165000 L: 326.288 norm:0.000088\n",
      "i: 165500 L: 326.281 norm:0.000088\n",
      "i: 166000 L: 326.274 norm:0.000088\n",
      "i: 166500 L: 326.267 norm:0.000087\n",
      "i: 167000 L: 326.259 norm:0.000087\n",
      "i: 167500 L: 326.252 norm:0.000087\n",
      "i: 168000 L: 326.245 norm:0.000087\n",
      "i: 168500 L: 326.238 norm:0.000086\n",
      "i: 169000 L: 326.231 norm:0.000086\n",
      "i: 169500 L: 326.224 norm:0.000086\n",
      "i: 170000 L: 326.217 norm:0.000086\n",
      "i: 170500 L: 326.211 norm:0.000085\n",
      "i: 171000 L: 326.204 norm:0.000085\n",
      "i: 171500 L: 326.197 norm:0.000085\n",
      "i: 172000 L: 326.190 norm:0.000085\n",
      "i: 172500 L: 326.183 norm:0.000084\n",
      "i: 173000 L: 326.177 norm:0.000084\n",
      "i: 173500 L: 326.170 norm:0.000084\n",
      "i: 174000 L: 326.163 norm:0.000084\n",
      "i: 174500 L: 326.157 norm:0.000083\n",
      "i: 175000 L: 326.150 norm:0.000083\n",
      "i: 175500 L: 326.143 norm:0.000083\n",
      "i: 176000 L: 326.137 norm:0.000083\n",
      "i: 176500 L: 326.130 norm:0.000082\n",
      "i: 177000 L: 326.124 norm:0.000082\n",
      "i: 177500 L: 326.117 norm:0.000082\n",
      "i: 178000 L: 326.111 norm:0.000082\n",
      "i: 178500 L: 326.105 norm:0.000082\n",
      "i: 179000 L: 326.098 norm:0.000081\n",
      "i: 179500 L: 326.092 norm:0.000081\n",
      "i: 180000 L: 326.086 norm:0.000081\n",
      "i: 180500 L: 326.080 norm:0.000081\n",
      "i: 181000 L: 326.073 norm:0.000080\n",
      "i: 181500 L: 326.067 norm:0.000080\n",
      "i: 182000 L: 326.061 norm:0.000080\n",
      "i: 182500 L: 326.055 norm:0.000080\n",
      "i: 183000 L: 326.049 norm:0.000080\n",
      "i: 183500 L: 326.043 norm:0.000079\n",
      "i: 184000 L: 326.037 norm:0.000079\n",
      "i: 184500 L: 326.031 norm:0.000079\n",
      "i: 185000 L: 326.025 norm:0.000079\n",
      "i: 185500 L: 326.019 norm:0.000078\n",
      "i: 186000 L: 326.013 norm:0.000078\n",
      "i: 186500 L: 326.007 norm:0.000078\n",
      "i: 187000 L: 326.001 norm:0.000078\n",
      "i: 187500 L: 325.995 norm:0.000078\n",
      "i: 188000 L: 325.989 norm:0.000077\n",
      "i: 188500 L: 325.983 norm:0.000077\n",
      "i: 189000 L: 325.978 norm:0.000077\n",
      "i: 189500 L: 325.972 norm:0.000077\n",
      "i: 190000 L: 325.966 norm:0.000077\n",
      "i: 190500 L: 325.960 norm:0.000076\n",
      "i: 191000 L: 325.955 norm:0.000076\n",
      "i: 191500 L: 325.949 norm:0.000076\n",
      "i: 192000 L: 325.943 norm:0.000076\n",
      "i: 192500 L: 325.938 norm:0.000076\n",
      "i: 193000 L: 325.932 norm:0.000075\n",
      "i: 193500 L: 325.927 norm:0.000075\n",
      "i: 194000 L: 325.921 norm:0.000075\n",
      "i: 194500 L: 325.916 norm:0.000075\n",
      "i: 195000 L: 325.910 norm:0.000075\n",
      "i: 195500 L: 325.905 norm:0.000074\n",
      "i: 196000 L: 325.899 norm:0.000074\n",
      "i: 196500 L: 325.894 norm:0.000074\n",
      "i: 197000 L: 325.889 norm:0.000074\n",
      "i: 197500 L: 325.883 norm:0.000074\n",
      "i: 198000 L: 325.878 norm:0.000073\n",
      "i: 198500 L: 325.873 norm:0.000073\n",
      "i: 199000 L: 325.867 norm:0.000073\n",
      "i: 199500 L: 325.862 norm:0.000073\n",
      "W1 matrix: \n",
      "[[ -3.94569338  -1.75329736 -10.35482363]\n",
      " [ -4.04534416  -1.77069082 -10.64246258]]\n",
      "b1 vector: \n",
      "[[0.37335154]\n",
      " [0.71720262]\n",
      " [1.6551463 ]]\n",
      "W2 matrix: \n",
      "[[ 5.25920207]\n",
      " [ 4.25734337]\n",
      " [13.56365085]]\n",
      "b2 vector: \n",
      "[[0.76618306]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "n_iter = 200000                        # Number of iterations\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2,3)/((2*3)**2)   # Weight matrix 1.\n",
    "b1 = np.random.randn(3,1)/((3*1)**2)   # Bias vector 1.\n",
    "W2 = np.random.randn(3,1)/((3*1)**2)   # Weight matrix 2.\n",
    "b2 = np.random.randn(1,1)/((1*1)**2)   # Bias vector 2.\n",
    "\n",
    "# We will keep track of training loss over iterations.\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, Y_train, W1, b1, W2, b2)]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # gradient descent \n",
    "    \n",
    "    gradient_W1, gradient_b1, gradient_W2, gradient_b2 = \\\n",
    "        L_prime(X_train, Y_train, W1, b1, W2, b2)\n",
    "    \n",
    "    W1_new = W1 - learning_rate * gradient_W1\n",
    "    b1_new = b1 - learning_rate * gradient_b1\n",
    "    W2_new = W2 - learning_rate * gradient_W2\n",
    "    b2_new = b2 - learning_rate * gradient_b2\n",
    "    \n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, Y_train, W1_new, b1_new, W2_new, b2_new))\n",
    "    \n",
    "    # L1-norm of weight/bias changing.\n",
    "    norm = np.abs(W1_new-W1).sum() + np.abs(b1_new-b1).sum() + \\\n",
    "           np.abs(W2_new-W2).sum() + np.abs(b2_new-b2).sum() \n",
    "    \n",
    "    if i%500 == 0:\n",
    "        print('i: {:6d} L: {:.3f} norm:{:.6f}'.format(i, L_list[-1], norm))\n",
    "        \n",
    "    W1 = W1_new\n",
    "    b1 = b1_new\n",
    "    W2 = W2_new\n",
    "    b2 = b2_new\n",
    "    \n",
    "print ('W1 matrix: \\n' + str(W1))\n",
    "print ('b1 vector: \\n' + str(b1))\n",
    "print ('W2 matrix: \\n' + str(W2))\n",
    "print ('b2 vector: \\n' + str(b2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how simple this is using keras :)\n",
    "# Try adding more layers, changing activation to e.g. 'tanh' or 'relu' or 'sigmoid' and compare results!\n",
    "# You may notice that \"linear\" works best and thats obvious because our data is fairly linear\n",
    "# You can also try changing the data generated\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='linear'),\n",
    "    # typically, more neurons, the more capable the network, but be wary of overfitting\n",
    "    tf.keras.layers.Dense(32, activation='linear'), \n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# we specify we want to use something known as the Adam optimizer to optimize the loss and minimize it\n",
    "# Adam, like SGD, tries to minimize the loss function. In a future workshop we will explain why Adam runs much \n",
    "# faster and has higher accuracy\n",
    "# the loss we use here is known as Mean Squared Error\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 0s 684us/step - loss: 9.2903e-13 - mse: 9.2903e-13\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 599us/step - loss: 5.1374e-12 - mse: 5.1374e-12\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 647us/step - loss: 1.7702e-12 - mse: 1.7702e-12\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 718us/step - loss: 7.0752e-13 - mse: 7.0752e-13\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 662us/step - loss: 9.6154e-11 - mse: 9.6154e-11\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.5730e-09 - mse: 1.5730e-09\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 653us/step - loss: 2.8780e-08 - mse: 2.8780e-08\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 647us/step - loss: 9.4780e-07 - mse: 9.4780e-07\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 588us/step - loss: 5.6141e-07 - mse: 5.6141e-07\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 680us/step - loss: 2.3260e-06 - mse: 2.3260e-06\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 621us/step - loss: 3.7253e-07 - mse: 3.7253e-07\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 678us/step - loss: 3.5856e-08 - mse: 3.5856e-08\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 857us/step - loss: 3.3026e-09 - mse: 3.3026e-09\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.5145e-10 - mse: 1.5145e-10\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 600us/step - loss: 1.9749e-11 - mse: 1.9749e-11\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 660us/step - loss: 2.1191e-12 - mse: 2.1191e-12\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 633us/step - loss: 1.2901e-13 - mse: 1.2901e-13\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 577us/step - loss: 1.5433e-14 - mse: 1.5433e-14\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 662us/step - loss: 5.6229e-15 - mse: 5.6229e-15\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 624us/step - loss: 1.8079e-14 - mse: 1.8079e-14\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 632us/step - loss: 6.4647e-15 - mse: 6.4647e-15\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 644us/step - loss: 4.4258e-15 - mse: 4.4258e-15\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 657us/step - loss: 5.7238e-15 - mse: 5.7238e-15\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 644us/step - loss: 5.9864e-15 - mse: 5.9864e-15\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 677us/step - loss: 3.8765e-15 - mse: 3.8765e-15\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 583us/step - loss: 6.0462e-15 - mse: 6.0462e-15\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 644us/step - loss: 4.5845e-15 - mse: 4.5845e-15\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 633us/step - loss: 5.1623e-15 - mse: 5.1623e-15\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 643us/step - loss: 1.4313e-13 - mse: 1.4313e-13\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 676us/step - loss: 3.6800e-14 - mse: 3.6800e-14\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 584us/step - loss: 2.0869e-13 - mse: 2.0869e-13\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 622us/step - loss: 3.3425e-13 - mse: 3.3425e-13\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 635us/step - loss: 4.3534e-14 - mse: 4.3534e-14\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 578us/step - loss: 7.1655e-15 - mse: 7.1655e-15\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 5.7968e-15 - mse: 5.7968e-15\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 620us/step - loss: 5.4481e-14 - mse: 5.4481e-14\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 668us/step - loss: 1.5548e-12 - mse: 1.5548e-12\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 640us/step - loss: 1.0882e-10 - mse: 1.0882e-10\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 597us/step - loss: 2.1040e-09 - mse: 2.1040e-09\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 613us/step - loss: 1.0547e-07 - mse: 1.0547e-07\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 634us/step - loss: 4.2171e-07 - mse: 4.2171e-07\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 569us/step - loss: 8.1457e-07 - mse: 8.1457e-07\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 706us/step - loss: 2.8790e-07 - mse: 2.8790e-07\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 663us/step - loss: 1.9426e-08 - mse: 1.9426e-08\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 666us/step - loss: 1.0392e-08 - mse: 1.0392e-08\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 670us/step - loss: 1.4062e-08 - mse: 1.4062e-08\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 658us/step - loss: 5.7670e-09 - mse: 5.7670e-09\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 640us/step - loss: 4.4165e-10 - mse: 4.4165e-10\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 616us/step - loss: 1.9954e-10 - mse: 1.9954e-10\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 663us/step - loss: 8.7907e-11 - mse: 8.7907e-11\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 763us/step - loss: 8.8047e-11 - mse: 8.8047e-11\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 705us/step - loss: 6.6314e-12 - mse: 6.6314e-12\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 606us/step - loss: 2.5876e-11 - mse: 2.5876e-11\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 770us/step - loss: 4.1245e-12 - mse: 4.1245e-12\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 652us/step - loss: 1.8879e-11 - mse: 1.8879e-11\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 617us/step - loss: 1.8297e-10 - mse: 1.8297e-10\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 618us/step - loss: 7.4875e-08 - mse: 7.4875e-08\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 624us/step - loss: 8.0851e-08 - mse: 8.0851e-08\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 611us/step - loss: 2.7584e-08 - mse: 2.7584e-08\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 589us/step - loss: 1.2610e-09 - mse: 1.2610e-09\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 583us/step - loss: 8.4368e-07 - mse: 8.4368e-07\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 622us/step - loss: 1.0077e-05 - mse: 1.0077e-05\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 563us/step - loss: 1.8524e-06 - mse: 1.8524e-06\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 674us/step - loss: 8.7230e-08 - mse: 8.7230e-08\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 644us/step - loss: 9.5395e-09 - mse: 9.5395e-09\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 588us/step - loss: 2.5927e-09 - mse: 2.5927e-09\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 598us/step - loss: 7.0628e-10 - mse: 7.0628e-10\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 631us/step - loss: 1.7601e-10 - mse: 1.7601e-10\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 608us/step - loss: 4.6870e-11 - mse: 4.6870e-11\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 619us/step - loss: 6.8526e-12 - mse: 6.8526e-12\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 653us/step - loss: 1.2505e-12 - mse: 1.2505e-12\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 617us/step - loss: 3.0134e-13 - mse: 3.0134e-13\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 608us/step - loss: 8.4756e-14 - mse: 8.4756e-14\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 603us/step - loss: 6.6470e-14 - mse: 6.6470e-14\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 720us/step - loss: 1.7651e-14 - mse: 1.7651e-14\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 628us/step - loss: 7.0433e-15 - mse: 7.0433e-15\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 599us/step - loss: 6.7994e-15 - mse: 6.7994e-15\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 592us/step - loss: 8.0657e-15 - mse: 8.0657e-15\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 619us/step - loss: 6.8489e-15 - mse: 6.8489e-15\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 602us/step - loss: 5.1319e-15 - mse: 5.1319e-15\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 602us/step - loss: 5.1398e-15 - mse: 5.1398e-15\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 583us/step - loss: 4.5098e-15 - mse: 4.5098e-15\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 603us/step - loss: 6.5194e-15 - mse: 6.5194e-15\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 638us/step - loss: 5.4523e-15 - mse: 5.4523e-15\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 653us/step - loss: 8.3466e-15 - mse: 8.3466e-15\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 775us/step - loss: 3.5661e-15 - mse: 3.5661e-15\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 620us/step - loss: 3.7109e-15 - mse: 3.7109e-15\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 606us/step - loss: 3.1702e-15 - mse: 3.1702e-15\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 671us/step - loss: 7.5906e-15 - mse: 7.5906e-15\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 603us/step - loss: 4.4700e-15 - mse: 4.4700e-15\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 587us/step - loss: 6.9547e-15 - mse: 6.9547e-15\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 579us/step - loss: 1.7581e-14 - mse: 1.7581e-14\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 598us/step - loss: 2.5375e-14 - mse: 2.5375e-14\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 636us/step - loss: 4.4482e-14 - mse: 4.4482e-14\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 617us/step - loss: 6.3659e-15 - mse: 6.3659e-15\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 645us/step - loss: 4.4888e-15 - mse: 4.4888e-15\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 627us/step - loss: 1.3759e-14 - mse: 1.3759e-14\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 593us/step - loss: 3.7084e-14 - mse: 3.7084e-14\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 637us/step - loss: 5.2471e-15 - mse: 5.2471e-15\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 667us/step - loss: 5.5572e-15 - mse: 5.5572e-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14eef29d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model onto our dataset and run for 100 epochs\n",
    "model.fit(X_train, Y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0.03923083 0.03530981], Y = [0.03727032], Predicted - [[0.03727032]]\n",
      "X = [0.00582148 0.03699618], Y = [0.02140883], Predicted - [[0.02140883]]\n",
      "X = [-0.03350683 -0.03459055], Y = [-0.03404869], Predicted - [[-0.0340487]]\n",
      "X = [-0.00560108  0.03889558], Y = [0.01664725], Predicted - [[0.01664725]]\n",
      "X = [ 0.06239572 -0.02463851], Y = [0.0188786], Predicted - [[0.01887859]]\n",
      "X = [ 0.00440151 -0.00883391], Y = [-0.0022162], Predicted - [[-0.00221621]]\n",
      "X = [0.05087184 0.0126296 ], Y = [0.03175072], Predicted - [[0.03175072]]\n",
      "X = [0.05500249 0.0538404 ], Y = [0.05442145], Predicted - [[0.05442144]]\n",
      "X = [ 0.0174499  -0.00063104], Y = [0.00840943], Predicted - [[0.00840943]]\n",
      "X = [0.04804347 0.0659605 ], Y = [0.05700198], Predicted - [[0.05700199]]\n",
      "X = [0.01794367 0.0351309 ], Y = [0.02653729], Predicted - [[0.02653729]]\n",
      "X = [0.00580836 0.09100084], Y = [0.0484046], Predicted - [[0.0484046]]\n",
      "X = [0.07179332 0.0032296 ], Y = [0.03751146], Predicted - [[0.03751146]]\n",
      "X = [0.04355678 0.05336648], Y = [0.04846163], Predicted - [[0.04846163]]\n",
      "X = [0.04802725 0.03115213], Y = [0.03958969], Predicted - [[0.03958969]]\n",
      "X = [0.0990629  0.03031729], Y = [0.06469009], Predicted - [[0.0646901]]\n",
      "X = [0.05647    0.02052422], Y = [0.03849711], Predicted - [[0.0384971]]\n",
      "X = [0.07543035 0.06411302], Y = [0.06977169], Predicted - [[0.06977169]]\n",
      "X = [0.0623539  0.02959276], Y = [0.04597333], Predicted - [[0.04597333]]\n",
      "X = [0.1112304  0.12307533], Y = [0.11715286], Predicted - [[0.1171529]]\n"
     ]
    }
   ],
   "source": [
    "# lets look at 20 data points and see how we do\n",
    "for x, y in zip(X_train[0:80:4], Y_train[0:80:4]):\n",
    "    print(\"X = {}, Y = {}, Predicted - {}\".format(x, y, model.predict([[x[0], x[1]]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
